{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-shifted Data\n",
    "\n",
    "In this tutorial, we show how to transform the time-series data in the following ways:\n",
    "* `split` time-series with a lot of data points into mutiple segments, and\n",
    "* `time shift` the above segments so that they have the same timestamps.\n",
    "\n",
    "The above transformations allow you to easily compare the data over the last hour/day/week over the previous intervals. This helps you understand if your system's current behavior continues to match the past behavior.\n",
    "\n",
    "**Note**: This tutorial reads in metric data from the Monitoring API, or a Google Cloud Storage bucket:\n",
    "* If the variable `'common_prefix'` is set, the data is read from the Monitoring API.\n",
    "* If the variable `'common_prefix'` is not set, the data is loaded from a shared Cloud Storage bucket. See [here](../Storage/Storage APIs.ipynb) to learn more about the Storage API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the monitoring module and set the default project\n",
    "\n",
    "If there is no default project set already, you must do so using 'set_datalab_project_id'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.datalab.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalab.stackdriver import monitoring as gcm\n",
    "\n",
    "set_datalab_project_id('ajhamilton-scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most common instance name prefixes\n",
    "\n",
    "The prefix from an instance name is calculated by splitting on the last '-' character. All instances with the same prefixes are grouped together to get the prefix counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foofoo', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Initialize the query for CPU utilization over the last week, and read in its metadata.\n",
    "query_cpu = gcm.Query('compute.googleapis.com/instance/cpu/utilization', hours=7*24)\n",
    "cpu_metadata = query_cpu.metadata()\n",
    "\n",
    "# Count the occurrences of each prefix, and display the top 5.\n",
    "instance_prefix_counts = collections.Counter(\n",
    "  timeseries.metric.labels['instance_name'].rsplit('-', 1)[0]\n",
    "  for timeseries in cpu_metadata)\n",
    "instance_prefix_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the instance name prefix to filter on\n",
    "\n",
    "In this cell, you can select an instance name prefix to filter on. If you do not set this variable, then the data is read from a Cloud Storage bucket.\n",
    "\n",
    "You can look at the most frequent prefix in the previous cell. It is recommended that you select a prefix with the following properties:\n",
    "* the instances have the CPU Utilization metric data for at least the last 5 days\n",
    "* the instances span multiple zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prefix specified. The data will be read from a Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "# Set this variable to read data from your own project.\n",
    "common_prefix = None # 'my-instance-prefix'\n",
    "\n",
    "if common_prefix is None:\n",
    "  print('No prefix specified. The data will be read from a Cloud Storage bucket.')\n",
    "else:\n",
    "  print('You selected the prefix: \"%s\"' % (common_prefix,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the time series data\n",
    "\n",
    "Based on the value of `'common_prefix'` in the previous cell, the time series data is loaded from the Monitoring API, or a shared Cloud Storage bucket.\n",
    "\n",
    "In both cases, we load the time series of the CPU Utilization metric over the last week, aggregated to hourly intervals per zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'StringIO'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1774fdcd3638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatalab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'StringIO'"
     ]
    }
   ],
   "source": [
    "import StringIO\n",
    "\n",
    "import pandas \n",
    "\n",
    "import datalab.storage as storage\n",
    "\n",
    "\n",
    "if common_prefix is None:\n",
    "  print('Reading in data from a Cloud Storage Bucket')\n",
    "  \n",
    "  # Initialize the bucket name, and item key.\n",
    "  bucket_name = 'cloud-datalab-samples'\n",
    "  per_zone_data = 'stackdriver-monitoring/timeseries/per-zone-weekly-20161010.csv'\n",
    "\n",
    "  # Load the CSV from the bucket, and intialize the dataframe using it.\n",
    "  per_zone_data_item = storage.Bucket(bucket_name).item(per_zone_data)\n",
    "  per_zone_data_string = StringIO.StringIO(per_zone_data_item.read_from())\n",
    "  per_zone_cpu_data = pandas.DataFrame.from_csv(per_zone_data_string)\n",
    "  \n",
    "else:\n",
    "  print('Reading in data from the Monitoring API')\n",
    "  \n",
    "  # Filter the query to instances with the specified prefix.\n",
    "  query_cpu = query_cpu.select_metrics(instance_name_prefix=common_prefix)\n",
    "\n",
    "  # Aggregate to hourly intervals per zone.\n",
    "  query_cpu = query_cpu.align(gcm.Aligner.ALIGN_MEAN, hours=1)\n",
    "  query_cpu = query_cpu.reduce(gcm.Reducer.REDUCE_MEAN, 'resource.zone')\n",
    "\n",
    "  # Get the time series data as a dataframe, with a single-level header.\n",
    "  per_zone_cpu_data = query_cpu.as_dataframe(label='zone')\n",
    "  \n",
    "per_zone_cpu_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into daily chunks\n",
    "\n",
    "Here, we split the data over daily boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Extract the number of days in the dataframe.\n",
    "num_days = len(per_zone_cpu_data.index)/24\n",
    "\n",
    "# Split the big dataframe into daily dataframes.\n",
    "daily_dataframes = [per_zone_cpu_data.iloc[24*i: 24*(i+1)]\n",
    "                    for i in xrange(num_days)]\n",
    "\n",
    "# Reverse the list to have today's data in the first index.\n",
    "daily_dataframes.reverse()\n",
    "\n",
    "# Display the last five rows from today's data.\n",
    "daily_dataframes[0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a helper function\n",
    "\n",
    "Here, we initialize a helper function to create human readable names for days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = 'Today'\n",
    "\n",
    "# Helper function to make a readable day name based on offset from today.\n",
    "def make_day_name(offset):\n",
    "  if offset == 0:\n",
    "    return TODAY\n",
    "  elif offset == 1:\n",
    "    return 'Yesterday'\n",
    "  return '%d days ago' % (offset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-shift all dataframes to line up with the last day\n",
    "\n",
    "The pandas method [tshift](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.tshift.html) lets you shift a dataframe by a specified offset. We use this to shift the index of all days to match the timestamps in the latest day.\n",
    "\n",
    "The data for each zone is inserted in a differenct dataframe, where the rows are timestamps and columns are specific days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zone names.\n",
    "all_zones = per_zone_cpu_data.columns.tolist()\n",
    "\n",
    "# Use the last day's timestamps as the index, and initialize a dataframe per zone.\n",
    "last_day_index = daily_dataframes[0].index\n",
    "zone_to_shifted_df = {zone: pandas.DataFrame([], index=last_day_index)\n",
    "                      for zone in all_zones}\n",
    "\n",
    "for i, dataframe in enumerate(daily_dataframes):\n",
    "  # Shift the dataframe to line up with the start of the last day.\n",
    "  dataframe = dataframe.tshift(freq=last_day_index[0] - dataframe.index[0])\n",
    "  current_day_name = make_day_name(i)\n",
    "  \n",
    "  # Insert each daily dataframe as a column into the dataframe.\n",
    "  for zone in all_zones:\n",
    "    zone_to_shifted_df[zone][current_day_name] = dataframe[zone]\n",
    "    \n",
    "# Display the first five rows from the first zone.    \n",
    "zone_to_shifted_df[all_zones[0]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the CPU utilization day-over-day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone, dataframe in zone_to_shifted_df.iteritems():\n",
    "  dataframe.plot(title=zone).legend(loc=\"upper left\", bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare today's CPU Utilization to the weekly average\n",
    "\n",
    "In order to compare the metric data for today, with the average of the week, we create new dataframes with the following columns:\n",
    "* Today's data: From the original data for TODAY\n",
    "* Average over the week: From the mean across all the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone, dataframe in zone_to_shifted_df.iteritems():\n",
    "  # Initialize the dataframe by extracting the column with data for today.\n",
    "  compare_to_avg_df = dataframe.loc[:, [TODAY]]\n",
    "  \n",
    "  # Add a column with the weekly avg.\n",
    "  compare_to_avg_df['Weekly avg.'] = dataframe.mean(axis=1)\n",
    "  \n",
    "  # Plot this dataframe.\n",
    "  compare_to_avg_df.plot(title=zone).legend(loc=\"upper left\", bbox_to_anchor=(1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
