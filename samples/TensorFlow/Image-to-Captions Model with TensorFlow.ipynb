{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Captions --- Show and Tell Model with Cats and Dogs\n",
    "\n",
    "Show and Tell Model --- Given an image, the model generates captions.\n",
    "\n",
    "![](https://storage.googleapis.com/bradley-sample-notebook-data/chopin_vivaldi_thumbnail.jpg)\n",
    "\n",
    "Generated captions:\n",
    "\n",
    "- a cat laying on top of a rug next to a cat\n",
    "- a cat laying on the floor next to a cat\n",
    "- a cat laying on top of a rug next to a cat\n",
    "\n",
    "\n",
    "\n",
    "The \"Show and Tell\" model presented in this notebook is based on work in https://github.com/tensorflow/models/tree/master/im2txt. Modifications are made to make training much faster (from one week with GPU to a few hours with CPU only). More specifically, the following modifications are made:\n",
    "\n",
    "1. Only cats and dogs images are used so only about 8% of data are used.\n",
    "2. We pre-generate inception embeddings once, instead of doing it during training to reduce the time it needs to generate embeddings from images. The quality is equivalent to the first phase in the github example above.\n",
    "\n",
    "The following diagram illustrates the model architecture (For details, see show and tell model on [github](https://github.com/tensorflow/models/tree/master/im2txt)).\n",
    "\n",
    "![](https://storage.googleapis.com/bradley-sample-notebook-data/show_and_tell_architecture.jpg)\n",
    "\n",
    "Send any feedback to datalab-feedback@google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "150 GB disk. n1-standard-1 VM is probably not enough. Recommend high-mem VM types. If you use \"datalab create\" command to create the Datalab instance, I would suggest high memory VMs by adding \"--machine-type n1-highmem-2\" option. See https://cloud.google.com/datalab/docs/how-to/machine-type for instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "\n",
    "We will use [MSCOCO](http://mscoco.org/) data. Although we only use the cats and dogs related images and captions, we need to download the zip packages with full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content/datalab/img2txt/images: Permission denied\n",
      "--2018-12-17 18:25:50--  http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
      "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 52.176.224.96\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13510573713 (13G) [application/octet-stream]\n",
      "/content/datalab/img2txt: No such file or directory\n",
      "/content/datalab/img2txt/train2014.zip: No such file or directory\n",
      "\n",
      "Cannot write to ‘/content/datalab/img2txt/train2014.zip’ (No such file or directory).\n",
      "--2018-12-17 18:25:51--  http://msvocds.blob.core.windows.net/coco2014/val2014.zip\n",
      "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 52.176.224.96\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6645013297 (6.2G) [application/octet-stream]\n",
      "/content/datalab/img2txt: No such file or directory\n",
      "/content/datalab/img2txt/val2014.zip: No such file or directory\n",
      "\n",
      "Cannot write to ‘/content/datalab/img2txt/val2014.zip’ (No such file or directory).\n",
      "unzip:  cannot find or open /content/datalab/img2txt/train2014.zip, /content/datalab/img2txt/train2014.zip.zip or /content/datalab/img2txt/train2014.zip.ZIP.\n",
      "unzip:  cannot find or open /content/datalab/img2txt/val2014.zip, /content/datalab/img2txt/val2014.zip.zip or /content/datalab/img2txt/val2014.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Download Images Data\n",
    "\n",
    "!mkdir -p /content/datalab/img2txt/images\n",
    "!wget -P /content/datalab/img2txt/ http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
    "!wget -P /content/datalab/img2txt/ http://msvocds.blob.core.windows.net/coco2014/val2014.zip\n",
    "!unzip -q -j /content/datalab/img2txt/train2014.zip -d /content/datalab/img2txt/images\n",
    "!unzip -q -j /content/datalab/img2txt/val2014.zip -d /content/datalab/img2txt/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-17 18:25:52--  http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
      "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 52.176.224.96\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19673183 (19M) [application/octet-stream Charset=UTF-8]\n",
      "/content/datalab/img2txt: No such file or directory\n",
      "/content/datalab/img2txt/captions_train-val2014.zip: No such file or directory\n",
      "\n",
      "Cannot write to ‘/content/datalab/img2txt/captions_train-val2014.zip’ (No such file or directory).\n",
      "unzip:  cannot find or open /content/datalab/img2txt/captions_train-val2014.zip, /content/datalab/img2txt/captions_train-val2014.zip.zip or /content/datalab/img2txt/captions_train-val2014.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Download Captions Data\n",
    "\n",
    "!wget -P /content/datalab/img2txt/ http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
    "!unzip -q -j /content/datalab/img2txt/captions_train-val2014.zip -d /content/datalab/img2txt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from random import randint\n",
    "import os\n",
    "import shutil\n",
    "import six\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\n",
    "from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(word_to_id, vocab_file):\n",
    "    \"\"\"Save vocabulary to file.\"\"\"\n",
    "\n",
    "    with tf.gfile.Open(vocab_file, 'w') as fw:\n",
    "        yaml.dump(word_to_id, fw, default_flow_style=False)\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Load vocabulary from file.\"\"\"\n",
    "    \n",
    "    with tf.gfile.Open(vocab_file, 'r') as fr:\n",
    "        return yaml.load(fr)  \n",
    "\n",
    "\n",
    "def get_instances_size(file_pattern):\n",
    "    \"\"\"Count training instances from tf.example file.\"\"\"\n",
    "\n",
    "    c = sum(1 for x in tf.python_io.tf_record_iterator(file_pattern))\n",
    "    print('instances size is %d' % c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCEPTION_V3_CHECKPOINT = 'gs://cloud-ml-data/img/flower_photos/inception_v3_2016_08_28.ckpt'\n",
    "INCEPTION_EXCLUDED_VARIABLES = ['InceptionV3/AuxLogits', 'InceptionV3/Logits', 'global_step']\n",
    "\n",
    "\n",
    "def make_batches(iterable, n):\n",
    "    \"\"\"Make batches with iterable.\"\"\"\n",
    "\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "\n",
    "def build_image_processing(image_str_tensor):\n",
    "    \"\"\"Create image-to-embeddings tf graph.\"\"\"\n",
    "\n",
    "    def _decode_and_resize(image_str_tensor):\n",
    "        \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n",
    "\n",
    "        # These constants are set by Inception v3's expectations.\n",
    "        height = 299\n",
    "        width = 299\n",
    "        channels = 3\n",
    "\n",
    "        image = tf.image.decode_jpeg(image_str_tensor, channels=channels)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n",
    "        image = tf.squeeze(image, squeeze_dims=[0])\n",
    "        image = tf.cast(image, dtype=tf.uint8)\n",
    "        return image\n",
    "\n",
    "    image = tf.map_fn(_decode_and_resize, image_str_tensor, back_prop=False, dtype=tf.uint8)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    inception_input = tf.multiply(image, 2.0)\n",
    "\n",
    "    # Build Inception layers, which expect a tensor of type float from [-1, 1)\n",
    "    # and shape [batch_size, height, width, channels].\n",
    "    with tf.contrib.slim.arg_scope(inception_v3_arg_scope()):\n",
    "        _, end_points = inception_v3(inception_input, is_training=False)    \n",
    "    embeddings = end_points['PreLogits']\n",
    "    inception_embeddings = tf.squeeze(embeddings, [1, 2], name='SpatialSqueeze')\n",
    "    return inception_embeddings\n",
    "\n",
    "\n",
    "def load_inception_checkpoint(sess, vars_to_restore, checkpoint_path=None):\n",
    "    \"\"\"Loal inception checkpoint to session.\"\"\"\n",
    "\n",
    "    saver = tf.train.Saver(vars_to_restore)\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_dir = tempfile.mkdtemp()\n",
    "        try:\n",
    "            checkpoint_tmp = os.path.join(checkpoint_dir, 'checkpoint')    \n",
    "            with tf.gfile.Open(INCEPTION_V3_CHECKPOINT, 'r') as f_in, tf.gfile.Open(checkpoint_tmp, 'w') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "            saver.restore(sess, checkpoint_tmp)\n",
    "        finally:\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    else:\n",
    "        saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabs, images files, captions that are only related to cats and dogs.\n",
    "\n",
    "from collections import Counter\n",
    "import six\n",
    "\n",
    "# If empty, all data is included. Otherwise, include only images with any of the words in its captions.\n",
    "KEYWORDS = {'cat', 'cats', 'kitten', 'kittens', 'dog', 'dogs', 'puppy', 'puppies'}\n",
    "\n",
    "# Sentence start, sentence end, and unknown word.\n",
    "CONTROL_WORDS = ['<s>', '</s>', '<unk>']\n",
    "\n",
    "\n",
    "def extract(train_content, val_content):\n",
    "    \"\"\"Extract vocab, captions, and image files from raw data.\n",
    "    \n",
    "    Returns:\n",
    "      A tuple of the following\n",
    "        - Vocab: in the form of word_to_id dict.\n",
    "        - id_wids: A dictionary with key an id, and value a list of captions, where each caption is\n",
    "                   represented by a list of word ids.\n",
    "        - id_imagefiles: A dictionary with key an id, and value a path of image file.\n",
    "    \"\"\"\n",
    "\n",
    "    id_captions = [(x['image_id'], x['caption']) for x in train_content['annotations']]\n",
    "    id_captions += [(x['image_id'], x['caption']) for x in val_content['annotations']]\n",
    "    id_captions = [(k, v.replace('.', '').replace(',', '').lower().split()) for k, v in id_captions]\n",
    "\n",
    "    # key - id, value - a list of captions\n",
    "    id_captions_filtered = {}\n",
    "    for x in id_captions:\n",
    "        if not KEYWORDS or (KEYWORDS & set(x[1])):\n",
    "            id_captions_filtered.setdefault(x[0], []).append(x[1])\n",
    "\n",
    "    print('number of captions is %d' % sum(len(x) for x in id_captions_filtered.values()))\n",
    "\n",
    "    words = [w for captions in id_captions_filtered.values() for caption in captions for w in caption]\n",
    "    counts = Counter(words)\n",
    "    counts = [x for x in counts.items() if x[1] > 5]\n",
    "    counts = sorted(counts, key=lambda x: (x[1]), reverse=True)\n",
    "    counts += [(x, 0) for x in CONTROL_WORDS]\n",
    "\n",
    "    word_to_id = {str(word_cnt_pair[0]): idx for idx, word_cnt_pair in enumerate(counts)}\n",
    "    print('vocab size is %d' % len(word_to_id))\n",
    "    \n",
    "    id_wids = {}\n",
    "    for k, v in six.iteritems(id_captions_filtered):\n",
    "        sentences = []\n",
    "        for caption in v:\n",
    "            wids = [word_to_id[x] if x in word_to_id else word_to_id['<unk>'] for x in caption]\n",
    "            wids = [word_to_id['<s>']] + wids + [word_to_id['</s>']]\n",
    "            sentences.append(wids)\n",
    "        id_wids[k] = sentences\n",
    "\n",
    "    id_imagefiles = {x['id']: x['file_name'] for x in train_content['images']}\n",
    "    id_imagefiles.update({x['id']: x['file_name'] for x in val_content['images']})    \n",
    "    id_imagefiles_filtered = {k: v for k, v in six.iteritems(id_imagefiles) if k in id_wids}\n",
    "    print('number of images is %d' % len(id_imagefiles_filtered))\n",
    "\n",
    "    return word_to_id, id_wids, id_imagefiles_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/datalab/img2txt/captions_val2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-950534591675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/datalab/img2txt/captions_val2014.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datalab/img2txt/captions_val2014.json'"
     ]
    }
   ],
   "source": [
    "# Load data from files.\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/content/datalab/img2txt/captions_val2014.json', 'r') as f:\n",
    "    val_content = json.load(f)\n",
    "\n",
    "with open('/content/datalab/img2txt/captions_train2014.json', 'r') as f:\n",
    "    train_content = json.load(f)\n",
    "\n",
    "word_to_id, id_wids, id_imagefiles = extract(train_content, val_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab so we can convert word ids to words in prediction.\n",
    "save_vocab(word_to_id, '/content/datalab/img2txt/vocab.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(id_imagefiles, id_wids, image_dir, output_dir, train_filename, eval_filename, test_filename, batch_size):\n",
    "    \"\"\"Convert images into embeddings, join with captions by id, splits results into train/eval/test,\n",
    "       and save to tf SequenceExample file.\n",
    "       \n",
    "       Note that train/eval data will be SequenceExample, but test data will be text\n",
    "       (a list of image file paths) because the final model expects raw images as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "        \n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature_list(values):\n",
    "        \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "        \n",
    "        return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "    tf.gfile.MakeDirs(output_dir)\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        image_str_tensor = tf.placeholder(tf.string, shape=None)\n",
    "        inception_embeddings = build_image_processing(image_str_tensor)\n",
    "        vars_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=INCEPTION_EXCLUDED_VARIABLES)\n",
    "        \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        load_inception_checkpoint(sess, vars_to_restore)\n",
    "\n",
    "        # Write to tf.example files.\n",
    "        train_file = os.path.join(output_dir, train_filename)        \n",
    "        eval_file = os.path.join(output_dir, eval_filename)\n",
    "        writer_train = tf.python_io.TFRecordWriter(train_file)        \n",
    "        writer_eval = tf.python_io.TFRecordWriter(eval_file)\n",
    "        writer_test = tf.gfile.Open(os.path.join(output_dir, test_filename), 'w')\n",
    "        batches = make_batches(list(six.iteritems(id_imagefiles)), batch_size)\n",
    "        num_of_batches = len(id_imagefiles) / batch_size + 1\n",
    "        for batch_num, b in enumerate(batches):\n",
    "            start = datetime.now()\n",
    "            image_bytes = []\n",
    "            for img in b:\n",
    "                with tf.gfile.Open(os.path.join(image_dir, img[1]), 'r') as f:\n",
    "                    image_bytes.append(f.read())\n",
    "\n",
    "            embs = sess.run(inception_embeddings, feed_dict={image_str_tensor: image_bytes}) \n",
    "            for img, emb in zip(b, embs):\n",
    "                rnd_num = randint(0, 100)\n",
    "                # 5% eval, 5% test, 90% training\n",
    "                if rnd_num > 4:\n",
    "                    writer = writer_train if rnd_num > 9 else writer_eval\n",
    "                    img_id = img[0]\n",
    "                    for caption_wids in id_wids[img_id]:\n",
    "                        context = tf.train.Features(feature={\"id\": _int64_feature(img_id), \"emb\": _float_feature(emb.tolist())})\n",
    "                        feature_lists = tf.train.FeatureLists(feature_list={\"wids\": _int64_feature_list(caption_wids)})\n",
    "                        sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)\n",
    "                        writer.write(sequence_example.SerializeToString())\n",
    "                else:\n",
    "                    writer_test.write('%d:%s\\n' % (img[0], img[1]))\n",
    "            elapsed = datetime.now() - start\n",
    "            print('processed batch %d of %d in %s' % (batch_num, num_of_batches, str(elapsed)))\n",
    "        writer_train.close()   \n",
    "        writer_eval.close()   \n",
    "        writer_test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(\n",
    "    id_imagefiles,\n",
    "    id_wids,\n",
    "    image_dir='/content/datalab/img2txt/images',\n",
    "    output_dir='/content/datalab/img2txt/transformed',\n",
    "    train_filename='train',\n",
    "    eval_filename='eval',\n",
    "    test_filename='test.txt',\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content/datalab/img2txt/transformed -l -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def parse_sequence_example(serialized):\n",
    "    \"\"\"Parses a tensorflow.SequenceExample into an image and caption.\n",
    "    Args:\n",
    "        serialized: A scalar string Tensor; a single serialized SequenceExample.\n",
    "    Returns:\n",
    "        id: a scalar integer Tensor.\n",
    "        emb: image embeddings, a 1-D Tensor with shape [2048].\n",
    "        wids: word ids, a 1-D Tensor with shape [None].\n",
    "    \"\"\"\n",
    "\n",
    "    context, sequence = tf.parse_single_sequence_example(\n",
    "        serialized,\n",
    "        context_features={\n",
    "            'id': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "            'emb': tf.FixedLenFeature([2048], dtype=tf.float32)\n",
    "        },\n",
    "        sequence_features={\n",
    "            'wids': tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        })\n",
    "\n",
    "    return context['id'], context['emb'], sequence['wids']\n",
    "\n",
    "\n",
    "def prefetch_input_data(file_pattern, batch_size):\n",
    "    \"\"\"Prefetches string values from disk vocab_idvocab_idinto an input queue.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: file patterns (e.g. /tmp/train_data-?????-of-00100).\n",
    "        batch_size: Model batch size used to determine queue capacity.\n",
    "    Returns:\n",
    "        A Queue containing prefetched string values.\n",
    "    \"\"\"\n",
    "\n",
    "    data_files = tf.gfile.Glob(file_pattern)\n",
    "    filename_queue = tf.train.string_input_producer(data_files, shuffle=True, capacity=16, name='filename_queue')\n",
    "    capacity = 1000 + 100 * batch_size\n",
    "    values_queue = tf.RandomShuffleQueue(\n",
    "        capacity=capacity,\n",
    "        min_after_dequeue=1000,\n",
    "        dtypes=[tf.string],\n",
    "        name=\"random_input_queue\")\n",
    "\n",
    "    enqueue_ops = []\n",
    "    reader = tf.TFRecordReader()    \n",
    "    _, value = reader.read(filename_queue)\n",
    "    enqueue_ops.append(values_queue.enqueue([value]))\n",
    "    tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(values_queue, enqueue_ops))\n",
    "\n",
    "    return values_queue\n",
    "\n",
    "\n",
    "def build_graph(serialized_sequence_example, vocab_size, train_batch_size, embedding_size, lstm_size, mode):\n",
    "    \"\"\" Build the main TensorFlow graph that will be shared by training and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    uniform_initializer = tf.random_uniform_initializer(minval=-0.08, maxval=0.08)\n",
    "    id, img_emb, wids = parse_sequence_example(serialized_sequence_example)\n",
    "    caption_length = tf.shape(wids)[0]\n",
    "    input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)\n",
    "    input_seq = tf.slice(wids, [0], input_length)\n",
    "    target_seq = tf.slice(wids, [1], input_length)\n",
    "    indicator = tf.ones(input_length, dtype=tf.int32)\n",
    "    enqueue_list = [[img_emb, input_seq, target_seq, indicator]]\n",
    "    img_embs, input_seqs, target_seqs, input_mask = tf.train.batch_join(\n",
    "        enqueue_list,\n",
    "        batch_size=train_batch_size,\n",
    "        capacity=train_batch_size * 2,\n",
    "        dynamic_pad=True,\n",
    "        name=\"batch_and_pad\")\n",
    "    \n",
    "    with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n",
    "        embedding_map = tf.get_variable(\n",
    "            name=\"map\",\n",
    "            shape=[vocab_size, embedding_size], initializer=uniform_initializer)\n",
    "        seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)\n",
    "\n",
    "    with tf.variable_scope(\"image_embedding\") as scope:\n",
    "        image_embeddings = tf.contrib.layers.fully_connected(\n",
    "            inputs=img_embs,\n",
    "            num_outputs=embedding_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=uniform_initializer,\n",
    "            biases_initializer=None,\n",
    "            scope=scope)        \n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size, state_is_tuple=True)\n",
    "    if mode == 'train':\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, input_keep_prob=0.7, output_keep_prob=0.7)\n",
    "\n",
    "    with tf.variable_scope(\"lstm\", initializer=tf.random_uniform_initializer(minval=-0.08, maxval=0.08)) as lstm_scope:\n",
    "        zero_state = lstm_cell.zero_state(batch_size=image_embeddings.get_shape()[0], dtype=tf.float32)\n",
    "        # Use image_embeddings as initial state.\n",
    "        _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "        lstm_scope.reuse_variables()\n",
    "        sequence_length = tf.reduce_sum(input_mask, 1)\n",
    "        lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                            inputs=seq_embeddings,\n",
    "                                            sequence_length=sequence_length,\n",
    "                                            initial_state=initial_state,\n",
    "                                            dtype=tf.float32,\n",
    "                                            scope=lstm_scope)\n",
    "        \n",
    "    # lstm_outputs's dim is [batch_size, max_seq_length, lstm_cell.output_size]\n",
    "    # Reshape it to 2D Tensor [batch * max_seq_length, lstm_cell.output_size] for loss computation.\n",
    "    lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "    with tf.variable_scope(\"logits\") as logits_scope:\n",
    "        logits = tf.contrib.layers.fully_connected(\n",
    "            inputs=lstm_outputs,\n",
    "            num_outputs=vocab_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=uniform_initializer,\n",
    "            scope=logits_scope)\n",
    "    \n",
    "    # Similarly, reshape targets to [batch * max_seq_length]\n",
    "    targets = tf.reshape(target_seqs, [-1])\n",
    "    \n",
    "    weights = tf.to_float(tf.reshape(input_mask, [-1]))\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "    batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)), tf.reduce_sum(weights), name=\"batch_loss\") \n",
    "    tf.summary.scalar(\"losses/batch_loss\", batch_loss)\n",
    "\n",
    "    global_step = tf.Variable(\n",
    "        initial_value=0,\n",
    "        name=\"global_step\",\n",
    "        trainable=False,\n",
    "        collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    \n",
    "    return batch_loss, losses, weights, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(vocab_size, train_batch_size, training_file_pattern, embedding_size=1024, lstm_size=512):\n",
    "    \"\"\"Build the training graph.\"\"\"\n",
    "    \n",
    "    train_instances_size = get_instances_size(training_file_pattern)\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        input_queue = prefetch_input_data(training_file_pattern, batch_size=train_batch_size)\n",
    "        serialized_sequence_example = input_queue.dequeue()\n",
    "        total_loss, _, _, global_step = build_graph(\n",
    "            serialized_sequence_example, vocab_size, train_batch_size, embedding_size, lstm_size, 'train')\n",
    "        learning_rate = tf.constant(2.0)   # initial_learning_rate\n",
    "        learning_rate_decay_factor = 0.5\n",
    "        num_batches_per_epoch = (train_instances_size / train_batch_size)\n",
    "        decay_steps = int(num_batches_per_epoch * 8)    # num_epochs_per_decay\n",
    "    \n",
    "        def _learning_rate_decay_fn(learning_rate, global_step):\n",
    "            return tf.train.exponential_decay(\n",
    "                learning_rate,\n",
    "                global_step,\n",
    "                decay_steps=decay_steps,\n",
    "                decay_rate=learning_rate_decay_factor,\n",
    "                staircase=True)\n",
    "      \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=total_loss,\n",
    "            global_step=global_step,\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer='SGD',\n",
    "            clip_gradients=5.0,\n",
    "            learning_rate_decay_fn=_learning_rate_decay_fn)   \n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "            \n",
    "    return g, train_op, global_step, saver\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous trained model\n",
    "!rm -r -f /content/datalab/img2txt/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab('/content/datalab/img2txt/vocab.yaml')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "graph, train_op, global_step, saver = train_graph(\n",
    "    vocab_size, \n",
    "    train_batch_size=64,\n",
    "    training_file_pattern='/content/datalab/img2txt/transformed/train')\n",
    "\n",
    "tf.contrib.slim.learning.train(\n",
    "    train_op,\n",
    "    '/content/datalab/img2txt/train',\n",
    "    log_every_n_steps=100,\n",
    "    graph=graph,\n",
    "    global_step=global_step,\n",
    "    number_of_steps=10000,\n",
    "    saver=saver)\n",
    "\n",
    "# Save inception checkpoint with the model.\n",
    "inception_checkpoint = os.path.join('/content/datalab/img2txt/train', 'inception_checkpoint')    \n",
    "with tf.gfile.Open(INCEPTION_V3_CHECKPOINT, 'r') as f_in, tf.gfile.Open(inception_checkpoint, 'w') as f_out:\n",
    "    f_out.write(f_in.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import Summary\n",
    "\n",
    "summary = Summary('/content/datalab/img2txt/train')\n",
    "summary.list_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.plot('losses/batch_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_graph(vocab_size, eval_batch_size, eval_file_pattern, embedding_size=1024, lstm_size=512):\n",
    "    \"\"\"Build evaluation graph.\"\"\"\n",
    "\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        input_queue = prefetch_input_data(eval_file_pattern, batch_size=eval_batch_size)\n",
    "        serialized_sequence_example = input_queue.dequeue()\n",
    "        _, losses, weights, global_step = build_graph(serialized_sequence_example, vocab_size, eval_batch_size, embedding_size, lstm_size, 'eval')\n",
    "        saver = tf.train.Saver()\n",
    "    return g, losses, weights, global_step, saver\n",
    "\n",
    "\n",
    "def eval_model(vocab_size, train_dir, eval_file_pattern, eval_batch_size=64):\n",
    "    \"\"\"Evaluate a trained model with evaluation data.\"\"\"\n",
    "    \n",
    "    eval_instances_size = get_instances_size(eval_file_pattern)\n",
    "    graph, losses, weights, global_step, saver = eval_graph(vocab_size, eval_batch_size=64, eval_file_pattern=eval_file_pattern)\n",
    "    checkpoint = tf.train.latest_checkpoint(train_dir)\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        global_step_val = tf.train.global_step(sess, global_step.name)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        num_eval_batches = int(math.ceil(eval_instances_size / eval_batch_size))\n",
    "        \n",
    "        sum_losses = 0.\n",
    "        sum_weights = 0.\n",
    "        for i in xrange(num_eval_batches):\n",
    "            losses_val, weights_val = sess.run([losses, weights])\n",
    "            sum_losses += np.sum(losses_val * weights_val)\n",
    "            sum_weights += np.sum(weights_val)\n",
    "            if i % 10 == 0:\n",
    "                tf.logging.info(\"Computed losses for %d of %d batches.\", i + 1, num_eval_batches)\n",
    "\n",
    "        perplexity = math.exp(sum_losses / sum_weights)\n",
    "        tf.logging.info(\"Perplexity = %f\", perplexity)\n",
    "        tf.logging.info(\"Finished processing evaluation at global step %d.\", global_step_val)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads, stop_grace_period_secs=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(vocab_size, '/content/datalab/img2txt/train', '/content/datalab/img2txt/transformed/eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "The prediction graph is mostly similar to train/eval graph, and they share all variables. The difference between them are:\n",
    "\n",
    "1. Prediction graph contains inception graph which converts image to embeddings. Therefore Prediction graph takes raw image as input.\n",
    "2. num_step is 1. The model output one word id each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def predict_graph(vocab_size, embedding_size=1024, lstm_size=512):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n",
    "        input_feed = tf.placeholder(dtype=tf.int64, shape=[None], name=\"input_feed\")        \n",
    "\n",
    "        images = tf.expand_dims(image_feed, 0)\n",
    "        input_seqs = tf.expand_dims(input_feed, 1)\n",
    "        \n",
    "        inception_embeddings = build_image_processing(images)\n",
    "        inception_vars = tf.contrib.slim.get_variables_to_restore(exclude=INCEPTION_EXCLUDED_VARIABLES)    \n",
    "        \n",
    "        with tf.variable_scope(\"seq_embedding\"):\n",
    "            embedding_map = tf.get_variable(\n",
    "                name=\"map\",\n",
    "                shape=[vocab_size, embedding_size])\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)\n",
    "        \n",
    "        with tf.variable_scope(\"image_embedding\") as scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                inputs=inception_embeddings,\n",
    "                num_outputs=embedding_size,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None,\n",
    "                scope=scope)\n",
    "\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size, state_is_tuple=True)            \n",
    "        with tf.variable_scope(\"lstm\") as lstm_scope:\n",
    "            zero_state = lstm_cell.zero_state(batch_size=image_embeddings.get_shape()[0], dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            initial_state = tf.concat(axis=1, values=initial_state)\n",
    "            lstm_scope.reuse_variables()        \n",
    "            tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "            state_feed = tf.placeholder(dtype=tf.float32, shape=[None, sum(lstm_cell.state_size)], name=\"state_feed\")\n",
    "            state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "            lstm_outputs, state_tuple = lstm_cell(inputs=tf.squeeze(seq_embeddings, axis=[1]), state=state_tuple)\n",
    "            lstm_state = tf.concat(axis=1, values=state_tuple, name=\"state\")            \n",
    "\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                inputs=lstm_outputs,\n",
    "                num_outputs=vocab_size,\n",
    "                activation_fn=None,\n",
    "                scope=logits_scope)\n",
    "        \n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "        trainable_vars = tf.contrib.slim.get_variables_to_restore(exclude=['InceptionV3/*'])\n",
    "        return g, image_feed, input_feed, state_feed, initial_state, lstm_state, softmax, inception_vars, trainable_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a \"beam search\": on one extreme, if max_caption_length is 20, we will have vocab^20 results and we will pick the one with greatest probs; on the other extreme, we pick only the top word for each step, and there will be only one result, which may not be the one with greatest probs. \"Beam search\" keeps track of top n paths for each step, and the final results will also be n predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Caption(object):\n",
    "    \"\"\"Represents a complete or partial caption.\"\"\"\n",
    "\n",
    "    def __init__(self, sentence, state, logprob, score, metadata=None):\n",
    "        \"\"\"Initializes the Caption.\n",
    "        Args:\n",
    "            sentence: List of word ids in the caption.\n",
    "            state: Model state after generating the previous word.\n",
    "            logprob: Log-probability of the caption.\n",
    "            score: Score of the caption.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentence = sentence\n",
    "        self.state = state\n",
    "        self.logprob = logprob\n",
    "        self.score = score\n",
    "\n",
    "    def __cmp__(self, other):\n",
    "        \"\"\"Compares Captions by score.\"\"\"\n",
    "        assert isinstance(other, Caption)\n",
    "        if self.score == other.score:\n",
    "            return 0\n",
    "        elif self.score < other.score:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "  \n",
    "    # For Python 3 compatibility (__cmp__ is deprecated).\n",
    "    def __lt__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score < other.score\n",
    "  \n",
    "    # Also for Python 3 compatibility.\n",
    "    def __eq__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score == other.score\n",
    "\n",
    "\n",
    "class TopN(object):\n",
    "    \"\"\"Maintains the top n elements of an incrementally provided set.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._n = n\n",
    "        self._data = []\n",
    "\n",
    "    def size(self):\n",
    "        assert self._data is not None\n",
    "        return len(self._data)\n",
    "\n",
    "    def push(self, x):\n",
    "        \"\"\"Pushes a new element.\"\"\"\n",
    "        assert self._data is not None\n",
    "        if len(self._data) < self._n:\n",
    "            heapq.heappush(self._data, x)\n",
    "        else:\n",
    "            heapq.heappushpop(self._data, x)\n",
    "\n",
    "    def extract(self, sort=False):\n",
    "        \"\"\"Extracts all elements from the TopN. This is a destructive operation.\n",
    "        The only method that can be called immediately after extract() is reset().\n",
    "        Args:\n",
    "          sort: Whether to return the elements in descending sorted order.\n",
    "        Returns:\n",
    "          A list of data; the top n elements provided to the set.\n",
    "        \"\"\"\n",
    "        assert self._data is not None\n",
    "        data = self._data\n",
    "        self._data = None\n",
    "        if sort:\n",
    "            data.sort(reverse=True)\n",
    "        return data\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns the TopN to an empty state.\"\"\"\n",
    "        self._data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "class ShowAndTellModel(object):\n",
    "    \n",
    "    def __init__(self, train_dir, vocab_file, max_caption_length=20, beam_size=5):\n",
    "        self._vocab = load_vocab(vocab_file)\n",
    "        self._train_dir = train_dir\n",
    "        self._max_caption_length = max_caption_length\n",
    "        self._beam_size = beam_size\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self._graph, self._image_feed, self._input_feed, self._state_feed, self._initial_state, \\\n",
    "            self._lstm_state, self._softmax, inception_vars, trainable_vars = predict_graph(len(self._vocab))\n",
    "        \n",
    "        self._sess = tf.Session(graph=self._graph)\n",
    "        \n",
    "        inception_checkpoint = os.path.join(self._train_dir, 'inception_checkpoint')\n",
    "        load_inception_checkpoint(self._sess, inception_vars, inception_checkpoint)\n",
    "        saver = tf.train.Saver(trainable_vars)\n",
    "        checkpoint_path = tf.train.latest_checkpoint(self._train_dir)\n",
    "        saver.restore(self._sess, checkpoint_path)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self._sess.close()\n",
    "        \n",
    "    def _process_results(self, captions):\n",
    "        id_to_word = {v: k for k, v in six.iteritems(self._vocab)}\n",
    "        for caption in captions:\n",
    "            words = [id_to_word[x] for x in caption.sentence]\n",
    "            words = filter(lambda x: x not in ['<s>', '</s>'], words)\n",
    "            yield ' '.join(words)\n",
    "        \n",
    "    def _predict(self, img_file):\n",
    "        \n",
    "        with tf.gfile.GFile(img_file, 'r') as f:\n",
    "            image_bytes = f.read()\n",
    "\n",
    "        init_state = self._sess.run(self._initial_state, feed_dict={self._image_feed: image_bytes})\n",
    "        initial_beam = Caption(sentence=[self._vocab['<s>']], state=init_state[0], logprob=0.0, score=0.0)\n",
    "        partial_captions = TopN(self._beam_size)\n",
    "        partial_captions.push(initial_beam)\n",
    "        complete_captions = TopN(self._beam_size)\n",
    "\n",
    "        # Run beam search.\n",
    "        for _ in range(self._max_caption_length - 1):\n",
    "            partial_captions_list = partial_captions.extract()\n",
    "            partial_captions.reset()\n",
    "            input_feed_val = np.array([c.sentence[-1] for c in partial_captions_list])\n",
    "            state_feed_val = np.array([c.state for c in partial_captions_list])\n",
    "        \n",
    "            softmax_val, new_states = self._sess.run([self._softmax, self._lstm_state],\n",
    "                                               feed_dict={self._input_feed: input_feed_val, self._state_feed: state_feed_val})\n",
    "\n",
    "            for i, partial_caption in enumerate(partial_captions_list):\n",
    "                word_probabilities = softmax_val[i]\n",
    "                state = new_states[i]\n",
    "                # For this partial caption, get the beam_size most probable next words.\n",
    "                words_and_probs = list(enumerate(word_probabilities))\n",
    "                words_and_probs.sort(key=lambda x: -x[1])\n",
    "                words_and_probs = words_and_probs[0:self._beam_size]\n",
    "                # Each next word gives a new partial caption.\n",
    "                for w, p in words_and_probs:\n",
    "                    if p < 1e-12:\n",
    "                        continue  # Avoid log(0).\n",
    "                    sentence = partial_caption.sentence + [w]\n",
    "                    logprob = partial_caption.logprob + math.log(p)\n",
    "                    score = logprob\n",
    "                    if w == self._vocab['</s>']:\n",
    "                        beam = Caption(sentence, state, logprob, score, None)\n",
    "                        complete_captions.push(beam)\n",
    "                    else:\n",
    "                        beam = Caption(sentence, state, logprob, score, None)\n",
    "                        partial_captions.push(beam)\n",
    "            if partial_captions.size() == 0:\n",
    "                # We have run out of partial candidates; happens when beam_size = 1.\n",
    "                break\n",
    "\n",
    "        # If we have no complete captions then fall back to the partial captions.\n",
    "        # But never output a mixture of complete and partial captions because a\n",
    "        # partial caption could have a higher score than all the complete captions.\n",
    "        if not complete_captions.size():\n",
    "            complete_captions = partial_captions\n",
    "\n",
    "        return complete_captions.extract(sort=True)\n",
    "\n",
    "\n",
    "    def show_and_tell(self, image_file): \n",
    "        with tf.gfile.GFile(image_file) as f:\n",
    "            img = Image.open(f)\n",
    "            img.thumbnail((299, 299), Image.ANTIALIAS)\n",
    "            display(img)\n",
    "        c = self._predict(image_file)\n",
    "        for r in self._process_results(c):\n",
    "            print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the first 10 instances from test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /content/datalab/img2txt/transformed/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ShowAndTellModel(train_dir='/content/datalab/img2txt/train',\n",
    "                      vocab_file='/content/datalab/img2txt/vocab.yaml') as m:\n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000165854.jpg')\n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_val2014_000000524382.jpg')\n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000524476.jpg')\n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000491728.jpg')\n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000033111.jpg')  \n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000344127.jpg')  \n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000169365.jpg')  \n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000098732.jpg')  \n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000557508.jpg')  \n",
    "    m.show_and_tell('/content/datalab/img2txt/images/COCO_train2014_000000492030.jpg')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, I would give it a try on pictures of my cats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ShowAndTellModel(train_dir='/content/datalab/img2txt/train',\n",
    "                      vocab_file='/content/datalab/img2txt/vocab.yaml') as m:\n",
    "    m.show_and_tell('gs://bradley-sample-notebook-data/chopin_vivaldi.jpg')\n",
    "    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi_chopin_tail.jpg')\n",
    "    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi.jpg')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "All our files are created under /content/datalab/img2txt. So just remove that dir to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
