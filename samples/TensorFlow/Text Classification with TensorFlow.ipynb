{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. The classification problem is to identify the newsgroup a post was summited to, given the text of the post.\n",
    "\n",
    "There are a few versions of this dataset from different sources online. Below, we use the version within scikit-learn which is already split into a train and test/eval set. For a longer introduction to this dataset, see the [scikit-learn website](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "\n",
    "This sequence of notebooks will write files to the file system under the WORKSPACE_PATH folder. Feel free to change this location in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content/datalab/workspace/tf/text_classification_20newsgroup: Permission denied\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/datalab/workspace/tf/text_classification_20newsgroup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b708ed125980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORKSPACE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datalab/workspace/tf/text_classification_20newsgroup'"
     ]
    }
   ],
   "source": [
    "# Note this path must be under /content/datalab in Datalab.\n",
    "# It is not recommended to use paths in /content/datalab/docs\n",
    "WORKSPACE_PATH = '/content/datalab/workspace/tf/text_classification_20newsgroup'\n",
    "!mkdir -p {WORKSPACE_PATH}\n",
    "\n",
    "import os\n",
    "os.chdir(WORKSPACE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data will be downloaded. Note that an error message saying something like \"No handlers could be found for \n",
    "# logger sklearn.datasets.twenty_newsgroups\" might be printed, but this is not an error.\n",
    "news_train_data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "news_test_data = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting and cleaning the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of labels/newsgroups within the training and test datasets are almost uniform. But more importantly, the distribution between test and training is about the same. Note that the first column is the integer id for the newsgroup while the 2nd column is the number of text examples with this newsgroup label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the 3rd element in the test dataset shows the data contains text with newlines, punctuation, misspellings, and other items common in text documents. To build a model, we will clean up the text by removing some of these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_data.data[2], news_train_data.target_names[news_train_data.target[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_text(news_data):\n",
    "    \"\"\"Cleans some issues with the text data\n",
    "    Args:\n",
    "        news_data: list of text strings\n",
    "    Returns:\n",
    "        For each text string, an array of tokenized words are returned in a list\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "    for text in news_data:\n",
    "        x = re.sub('[^\\w]|_', ' ', text)  # only keep numbers and letters and spaces\n",
    "        x = x.lower()\n",
    "        x = re.sub(r'[^\\x00-\\x7f]',r'', x)  # remove non ascii texts\n",
    "        tokens = [y for y in x.split(' ') if y] # remove empty words\n",
    "        tokens = ['[number]' if x.isdigit() else x for x in tokens]\n",
    "\n",
    "        # As an exercise, try stemming each token using python's nltk package.\n",
    "        cleaned_text.append(tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_tokens = clean_and_tokenize_text(news_train_data.data)\n",
    "clean_test_tokens = clean_and_tokenize_text(news_test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens_per_row(text_token_list):\n",
    "    \"\"\"Collect unique tokens per row.\n",
    "    Args:\n",
    "        text_token_list: list, where each element is a list containing tokenized text\n",
    "    Returns:\n",
    "        One list containing the unique tokens in every row. For example, if row one contained\n",
    "        ['pizza', 'pizza'] while row two contained ['pizza', 'cake', 'cake'], then the output list\n",
    "        would contain ['pizza' (from row 1), 'pizza' (from row 2), 'cake' (from row 2)]\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for row in text_token_list:\n",
    "        words.extend(list(set(row)))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot where the x-axis is a token, and the y-axis is how many text documents\n",
    "# that token is in. \n",
    "words = pd.DataFrame(get_unique_tokens_per_row(clean_train_tokens) , columns=['words'])\n",
    "token_frequency = words['words'].value_counts() # how many documents contain each token.\n",
    "token_frequency.plot(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of our tokens only appear in 1 document, while some appears almost every document. To build a good model, we should remove these low frequency tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_train_data.data), len(token_frequency)  # There are many more tokens than examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tokens.\n",
    "vocab = token_frequency[token_frequency > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.plot(logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six \n",
    "\n",
    "CONTROL_WORDS = ['<s>', '</s>', '<unk>']\n",
    "\n",
    "vocab_id = {v[0]: idx + 1 for idx, v in enumerate(sorted(six.iteritems(vocab), key=lambda x: x[1], reverse=True))}\n",
    "for c in CONTROL_WORDS:\n",
    "  vocab_id[c] = len(vocab_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_by_vocab(news_data, vocab_id):\n",
    "    \"\"\"Removes tokens if not in vocab.\n",
    "    Args:\n",
    "        news_data: list, where each element is a token list\n",
    "        vocab: set containing the tokens to keep.\n",
    "    Returns:\n",
    "        List of strings containing the final cleaned text data\n",
    "    \"\"\"\n",
    "    wids_all = []\n",
    "    for row in news_data:\n",
    "        wids = [vocab_id[token] if (token in vocab_id) else vocab_id['<unk>'] for token in row]\n",
    "        wids = [vocab_id['<s>']] + wids + [vocab_id['</s>']]\n",
    "        wids = wids[:128]\n",
    "        wids_all.append(wids)\n",
    "    return wids_all\n",
    "  \n",
    "clean_train_data = filter_text_by_vocab(clean_train_tokens, vocab_id)\n",
    "clean_test_data = filter_text_by_vocab(clean_test_tokens, vocab_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a check, let's make sure we didn't remove any data rows.\n",
    "len(clean_train_data), len(news_train_data.data), len(clean_test_data), len(news_test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "len(vocab_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_wids(wids, length):\n",
    "    \"\"\"Pad each instance to \"\"\"\n",
    "    padded = []\n",
    "    for r in wids:\n",
    "        if len(r) >= length:\n",
    "            padded.append(r[0:length])\n",
    "        else:\n",
    "            padded.append(r + [0] * (length - len(r)))\n",
    "    return padded\n",
    "  \n",
    "padded_train_data = pad_wids(clean_train_data, 128)\n",
    "padded_test_data = pad_wids(clean_test_data, 128)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DNN model\n",
    "\n",
    "We'll first build a simple DNN model with only 3 layers: input, embeddings, and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "from google.datalab.ml import Summary\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 512\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(batch_size, train_data, targets, mode):\n",
    "    \"\"\"Build an DNN Model. \"\"\"\n",
    "    \n",
    "    with tf.name_scope(mode):\n",
    "        raw_data = tf.convert_to_tensor(train_data, dtype=tf.int64)\n",
    "        targets = tf.convert_to_tensor(targets, dtype=tf.int64)\n",
    "        batch_num = len(train_data) // batch_size - 1\n",
    "        i = tf.train.range_input_producer(batch_num, shuffle=True).dequeue()\n",
    "        input_seqs = raw_data[i * batch_size: (i + 1) * batch_size]\n",
    "        targets = targets[i * batch_size: (i + 1) * batch_size]\n",
    "        length = tf.count_nonzero(input_seqs, axis=1, dtype=tf.int32)\n",
    "\n",
    "    embedding_map = tf.get_variable(\n",
    "        name=\"embeddings_map\",\n",
    "        shape=[len(vocab_id), EMBEDDING_SIZE])\n",
    "    seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)\n",
    "    \n",
    "    # Simply combine embeddings.\n",
    "    combined = tf.sqrt(tf.reduce_sum(tf.square(seq_embeddings), 1))\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        inputs=combined,\n",
    "        num_outputs=20,\n",
    "        activation_fn=None)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "    losses= tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    _, accuracy = tf.contrib.metrics.streaming_accuracy(targets, predictions)\n",
    "    correct_predictions = tf.count_nonzero(tf.equal(predictions, targets))\n",
    "    return losses, accuracy, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_fn, train_steps, model_dir):\n",
    "    \"\"\"Model trainer.\"\"\"\n",
    "\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        uniform_initializer = tf.random_uniform_initializer(minval=-0.08, maxval=0.08)\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=uniform_initializer):\n",
    "            losses_train, _, _ = model_fn(TRAIN_BATCH_SIZE, padded_train_data, news_train_data.target, 'train')\n",
    "        with tf.variable_scope(\"Model\", reuse=True):\n",
    "            _, accuracy, correct_predictions = model_fn(EVAL_BATCH_SIZE, padded_test_data, news_test_data.target, 'eval')\n",
    "\n",
    "        tf.summary.scalar('accuracy', accuracy)        \n",
    "        tf.summary.scalar('losses', losses_train)  \n",
    "        merged = tf.summary.merge_all()        \n",
    "        \n",
    "        global_step = tf.Variable(\n",
    "            initial_value=0,\n",
    "            name=\"global_step\",\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=losses_train,\n",
    "            global_step=global_step,\n",
    "            learning_rate=0.001,\n",
    "            optimizer='Adam')\n",
    "\n",
    "    def train_step_fn(sess, *args, **kwargs):\n",
    "        total_loss, should_stop = tf.contrib.slim.python.slim.learning.train_step(sess, *args, **kwargs)\n",
    "\n",
    "        if train_step_fn.train_steps % 50 == 0:\n",
    "            summary = sess.run(merged)\n",
    "            train_step_fn.eval_writer.add_summary(summary, train_step_fn.train_steps)\n",
    "            total_correct_predictions = 0\n",
    "            num_eval_batches = len(padded_test_data) / EVAL_BATCH_SIZE\n",
    "            for i in range(len(padded_test_data) / EVAL_BATCH_SIZE):\n",
    "                total_correct_predictions += sess.run(correct_predictions)\n",
    "            print('accuracy: %.4f' % (float(total_correct_predictions)/(num_eval_batches*EVAL_BATCH_SIZE)))\n",
    "\n",
    "        train_step_fn.train_steps += 1\n",
    "        return [total_loss, should_stop] \n",
    "\n",
    "    train_step_fn.train_steps = 0\n",
    "    train_step_fn.eval_writer = tf.summary.FileWriter(os.path.join(model_dir, 'eval'))\n",
    "\n",
    "    tf.contrib.slim.learning.train(\n",
    "        train_op,\n",
    "        model_dir,\n",
    "        graph=g,\n",
    "        global_step=global_step,\n",
    "        number_of_steps=train_steps,\n",
    "        log_every_n_steps=50,  \n",
    "        train_step_fn=train_step_fn)\n",
    "    train_step_fn.eval_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from fresh. Note that you can skip this step to continue training from previous checkpoint.\n",
    "!rm -rf dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dnn_model, train_steps=1501, model_dir='dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary('dnn/eval')\n",
    "summary.plot(['losses', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a bidirectional LSTM model\n",
    "\n",
    "Let's try an LSTM based sequential model and see if it can beat DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE=128\n",
    "\n",
    "def lstm_model(batch_size, train_data, targets, mode):\n",
    "    \"\"\"Build an LSTM Model. \"\"\"\n",
    "    \n",
    "    with tf.name_scope(mode):\n",
    "        raw_data = tf.convert_to_tensor(train_data, dtype=tf.int64)\n",
    "        targets = tf.convert_to_tensor(targets, dtype=tf.int64)\n",
    "        batch_num = len(train_data) // batch_size - 1\n",
    "        i = tf.train.range_input_producer(batch_num, shuffle=True).dequeue()\n",
    "        input_seqs = raw_data[i * batch_size: (i + 1) * batch_size]\n",
    "        targets = targets[i * batch_size: (i + 1) * batch_size]\n",
    "        length = tf.count_nonzero(input_seqs, axis=1, dtype=tf.int32)\n",
    "        \n",
    "    embedding_map = tf.get_variable(\n",
    "        name=\"embeddings_map\",\n",
    "        shape=[len(vocab_id), EMBEDDING_SIZE])\n",
    "    seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)\n",
    "\n",
    "    # This section is different from DNN model function.\n",
    "    #===================================================\n",
    "    lstm_cellf = tf.contrib.rnn.BasicLSTMCell(num_units=LSTM_SIZE)\n",
    "    lstm_cellb = tf.contrib.rnn.BasicLSTMCell(num_units=LSTM_SIZE)   \n",
    "\n",
    "    lstm_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=lstm_cellf,\n",
    "        cell_bw=lstm_cellb,\n",
    "        inputs=seq_embeddings,\n",
    "        dtype=tf.float32)\n",
    "    \n",
    "    lstm_outputs = tf.concat(lstm_outputs, 2)\n",
    "    indices = tf.range(tf.shape(length)[0])\n",
    "    slices = tf.stack([indices, length-1], axis=1)    \n",
    "    lstm_outputs = tf.gather_nd(lstm_outputs, indices=slices)    \n",
    "    #===================================================\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        inputs=lstm_outputs,\n",
    "        num_outputs=20,\n",
    "        activation_fn=None)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "    losses= tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    _, accuracy = tf.contrib.metrics.streaming_accuracy(targets, predictions)    \n",
    "    correct_predictions = tf.count_nonzero(tf.equal(predictions, targets))\n",
    "    return losses, accuracy, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from fresh. Note that you can skip this step to continue training from previous checkpoint.\n",
    "!rm -rf lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same trainer. Note that the training steps is greater based on experiments and\n",
    "# training time is much longer.\n",
    "train(lstm_model, train_steps=1501, model_dir='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary('lstm/eval')\n",
    "summary.plot(['losses', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model Performance\n",
    "\n",
    "With DNN, we aggregate the words embeddings in each sentence. This appears more efficient and accurate. With LSTM, we have an overfitting problem. With both models produce the same loss value towards the end of the training, the accuracy on eval data is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary(['dnn/eval', 'lstm/eval'])\n",
    "summary.plot(['losses', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Perhaps the results of LSTM model may get close to that of DNN with more training data. Also, it will probably helpful if we add a convlutional layer before LSTM layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
