{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook walks you through the creation of an LSTM (Long Short Term Memory) model using [TensorFlow](https://www.tensorflow.org/). The model can be used to insert punctuations automatically on paragraphs without punctuations. For example, given:\n",
    "\n",
    "*i think it is a report which will for the most part be supported by my group*\n",
    "\n",
    "It produces:\n",
    "\n",
    "*i think it is a report which will , for the most part , be supported by my group . *\n",
    "\n",
    "An imaginary usage of the model is for typing --- you can type a bunch of words and let it insert puncuations for you. It may also be used in speech recognition.\n",
    "\n",
    "The model does not rely on capitalization. All training and prediction data are converted to lowercase during data preparation. \n",
    "\n",
    "Send any feedback to datalab-feedback@google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "The training data used are [europarl](http://www.statmt.org/europarl/) and [comtran](http://www.fask.uni-mainz.de/user/rapp/comtrans/) from [NLTK Corpora](http://www.nltk.org/nltk_data/). I think both are extracted from the proceedings of the European Parliament. I chose these two datasets because first they have clean punctuations, and second they are large enough to create a decent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content/datalab/punctuation/tmp: Permission denied\n",
      "mkdir: /content/datalab/punctuation/data: Permission denied\n",
      "mkdir: /content/datalab/punctuation/datapreped: Permission denied\n",
      "unzip:  cannot find or open /content/datalab/punctuation/tmp/europarl_raw.zip, /content/datalab/punctuation/tmp/europarl_raw.zip.zip or /content/datalab/punctuation/tmp/europarl_raw.zip.ZIP.\n",
      "unzip:  cannot find or open /content/datalab/punctuation/tmp/comtrans.zip, /content/datalab/punctuation/tmp/comtrans.zip.zip or /content/datalab/punctuation/tmp/comtrans.zip.ZIP.\n",
      "cp: /content/datalab/punctuation/tmp/europarl_raw/english/*: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip data.\n",
    "\n",
    "!mkdir -p /content/datalab/punctuation/tmp\n",
    "!mkdir -p /content/datalab/punctuation/data\n",
    "!mkdir -p /content/datalab/punctuation/datapreped\n",
    "!wget -q -P /content/datalab/punctuation/tmp/ https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/europarl_raw.zip\n",
    "!wget -q -P /content/datalab/punctuation/tmp/ https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/comtrans.zip\n",
    "!unzip -q -o /content/datalab/punctuation/tmp/europarl_raw.zip -d /content/datalab/punctuation/tmp\n",
    "!unzip -q -o /content/datalab/punctuation/tmp/comtrans.zip -d /content/datalab/punctuation/tmp\n",
    "!cp /content/datalab/punctuation/tmp/europarl_raw/english/* /content/datalab/punctuation/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/datalab/punctuation/tmp/comtrans/alignment-en-fr.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a68faeccfb90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We only need English from `comtran` data. Extract English text only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/datalab/punctuation/tmp/comtrans/alignment-en-fr.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/datalab/punctuation/data/comtrans.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datalab/punctuation/tmp/comtrans/alignment-en-fr.txt'"
     ]
    }
   ],
   "source": [
    "# We only need English from `comtran` data. Extract English text only.\n",
    "with open('/content/datalab/punctuation/tmp/comtrans/alignment-en-fr.txt', 'r') as f_in, \\\n",
    "    open('/content/datalab/punctuation/data/comtrans.txt', 'w') as f_out:\n",
    "  num_lines = 0\n",
    "  for l in f_in.readlines():\n",
    "    if num_lines == 0:\n",
    "      f_out.write(l)\n",
    "    num_lines = (0 if num_lines == 2 else num_lines + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare data by cleaning up text.\"\"\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from random import randint\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def prep_data(corpora_path, out_dir):\n",
    "  \"\"\"Clean up raw data and split them into train, validation, and test source.\"\"\"\n",
    "  \n",
    "  printable = set(string.printable)\n",
    "  all_corpora_files = glob.glob(corpora_path)\n",
    "  lines = []\n",
    "\n",
    "  for corpora_file in all_corpora_files:\n",
    "    with open(corpora_file, 'r') as f:\n",
    "      lines += f.readlines()\n",
    "  \n",
    "  dest_train = os.path.join(out_dir, 'train.txt') \n",
    "  dest_valid = os.path.join(out_dir, 'valid.txt')  \n",
    "  dest_test = os.path.join(out_dir, 'test.txt') \n",
    "\n",
    "  valid_lines = 0\n",
    "  test_lines = 0\n",
    "  train_lines = 0\n",
    "\n",
    "  with open(dest_train, 'w') as f_train, open(dest_valid, 'w') as f_valid, open(dest_test, 'w') as f_test:\n",
    "    for l in lines:\n",
    "      s = l.strip()\n",
    "      # Remove \"bad\" sentences.\n",
    "      if s.endswith(')') and s.startswith('('):\n",
    "        continue\n",
    "      if not s.endswith('.') and not s.endswith('!') and not s.endswith('?'):\n",
    "        continue\n",
    "      if s.find('...') != -1:\n",
    "        continue\n",
    "\n",
    "      # Remove quotes, apostrophes, leading dashes.        \n",
    "      s = re.sub('\"', '', s)\n",
    "      s = re.sub(' \\' s ', 's ', s)   \n",
    "      s = re.sub('\\'', '', s)\n",
    "      s = re.sub('^- ', '', s)\n",
    "      \n",
    "      # Clean double punctuations.\n",
    "      s = re.sub('\\? \\.', '\\?', s)    \n",
    "      s = re.sub('\\! \\.', '\\!', s)\n",
    "      \n",
    "      # Extract human names to reduce vocab size. There are many names like 'Mrs Plooij-van Gorsel'\n",
    "      # 'Mr Cox'.\n",
    "      s = re.sub('Mr [\\w]+ [A-Z][\\w]+ ', '[humanname] ', s)\n",
    "      s = re.sub('Mrs [\\w]+ [A-Z][\\w]+ ', '[humanname] ', s)\n",
    "      s = re.sub('Mr [\\w]+ ', '[humanname] ', s)\n",
    "      s = re.sub('Mrs [\\w]+ ', '[humanname] ', s)\n",
    "      \n",
    "      # Remove brackets and contents inside.\n",
    "      s = re.sub('\\(.*\\) ', '', s)\n",
    "      s = re.sub('\\(', '', s)\n",
    "      s = re.sub('\\)', '', s)\n",
    "      \n",
    "      # Extract numbers to reduce the vocab size.\n",
    "      s = re.sub('[0-9\\.]+ ', '[number] ', s)  \n",
    "      \n",
    "      # Replace i.e., p.m., a.m. to reduce confusion on period.\n",
    "      s = re.sub(' i\\.e\\.', ' for example', s)          \n",
    "      s = re.sub(' p\\.m\\.', ' pm', s)   \n",
    "      s = re.sub(' a\\.m\\.', ' am', s) \n",
    "      \n",
    "      # Remove unprintable characters.\n",
    "      s = filter(lambda x: x in printable, s)\n",
    "      \n",
    "      s = s.lower()\n",
    "      \n",
    "      # For every 3 sentences we cut a new line to simulate a paragraph.\n",
    "      # Produce train/validation/test sets by 20:2:78\n",
    "      r = randint(0,50)\n",
    "      if r < 10:\n",
    "        valid_lines += 1\n",
    "        sep = '\\n' if (valid_lines % 3) == 0 else ' '\n",
    "        f_valid.write(s + sep)\n",
    "      elif r == 11:\n",
    "        test_lines += 1\n",
    "        sep = '\\n' if (test_lines % 3) == 0 else ' '\n",
    "        f_test.write(s + sep)\n",
    "      else:\n",
    "        train_lines += 1\n",
    "        sep = '\\n' if (train_lines % 3) == 0 else ' '\n",
    "        f_train.write(s + sep)\n",
    "\n",
    "\n",
    "prep_data('/content/datalab/punctuation/data/*', '/content/datalab/punctuation/datapreped')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Some of the code is ported from TensorFlow model [PTB Language Model](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We deal with limited punctuations only because of limited training data.\n",
    "PUNCTUATIONS = (u'.', u',', u'?', u'!', u':')\n",
    "# `n` means no punctuation.\n",
    "TARGETS = list(PUNCTUATIONS) + ['n']\n",
    "# Set vocab size to remove low frequency words. Roughly with 10000 vocab, words with less than three counts are excluded.\n",
    "VOCAB_SIZE = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions for reading input data.\"\"\"\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_words(filename):\n",
    "  \"\"\"Read words from file.\n",
    "  Args:\n",
    "    filename: path to the file to read words from.\n",
    "  Returns:\n",
    "    Words split by white space.\n",
    "  \"\"\"\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    x = f.read().decode(\"utf-8\").replace(\"\\n\", \" <eos> \").split()\n",
    "  if x[-1] != '<eos>':\n",
    "    x.append('<eos>')\n",
    "\n",
    "  indices = [i for i, w in enumerate(x) if w in PUNCTUATIONS]\n",
    "  # The next word after a punctuation is an important signal. We switch the punctuation\n",
    "  # with next word so it can be used as part of the context.\n",
    "  for i in indices:\n",
    "    x[i], x[i+1] = x[i+1], x[i]\n",
    "  return x\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "  \"\"\"Build vocabulary from training data file.\n",
    "  Args:\n",
    "    filename: path to the file to read words from.\n",
    "  Returns:\n",
    "    A dict with key being words and value being indices.\n",
    "  \"\"\"\n",
    "  x = read_words(filename)\n",
    "  counter = collections.Counter(x)\n",
    "  count_pairs = sorted(counter.items(), key=lambda a: (-a[1], a[0]))\n",
    "  count_pairs = count_pairs[:VOCAB_SIZE-1]\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "  word_to_id['<unk>'] = VOCAB_SIZE - 1\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_and_punc_ids(filename, word_to_id):\n",
    "  \"\"\"Produce indices from words in file. x are indices for words, and y are indices for punctuations.\n",
    "  Args:\n",
    "    filename: path to the file to read words from.\n",
    "    word_to_id: the vocab to indices dict.\n",
    "  Returns:\n",
    "    A pair. First element is the words indices. Second element is the target punctuation indices.\n",
    "  \"\"\"\n",
    "  x_words = read_words(filename)\n",
    "  x_id = [word_to_id[w] if w in word_to_id else word_to_id['<unk>'] for w in x_words]\n",
    "  target_to_id = {p:i for i, p in enumerate(TARGETS)}\n",
    "  y_words = x_words[1:] + ['padding']\n",
    "  y_puncts = ['n' if elem not in PUNCTUATIONS else elem for elem in y_words]\n",
    "  y_id = [target_to_id[p] for p in y_puncts]\n",
    "  return x_id, y_id\n",
    "\n",
    "\n",
    "def content_to_word_ids(content, word_to_id):\n",
    "  \"\"\"Produce indices from words from a given string.\n",
    "  Args:\n",
    "    filename: path to the file to read words from.\n",
    "    word_to_id: the vocab to indices dict.    \n",
    "  Returns:\n",
    "    Words indices.\n",
    "  \"\"\"\n",
    "  x = content.decode(\"utf-8\").replace(\"\\n\", \" <eos> \").split()\n",
    "  indices = [i for i, w in enumerate(x) if w in PUNCTUATIONS]\n",
    "  for i in indices:\n",
    "    x[i], x[i+1] = x[i+1], x[i]\n",
    "\n",
    "  x_id = [word_to_id[w] if w in word_to_id else word_to_id['<unk>'] for w in x]\n",
    "  return x_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The training model. \"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "\n",
    "class TrainingConfig(object):\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 50\n",
    "  hidden_size = 150\n",
    "  max_epoch =20\n",
    "  max_max_epoch = 25\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.7\n",
    "  batch_size = 100\n",
    "\n",
    "\n",
    "class TrainingInput(object):\n",
    "  \"\"\"The input data producer.\"\"\"\n",
    "\n",
    "  def _make_input_producer(self, raw_data, batch_size, num_steps, name=None):\n",
    "    with tf.name_scope(name, \"InputProducer\"):\n",
    "      raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "      data_len = tf.size(raw_data)\n",
    "      batch_len = data_len // batch_size\n",
    "      data = tf.reshape(raw_data[0 : batch_size * batch_len], [batch_size, batch_len])\n",
    "\n",
    "      epoch_size = (batch_len - 1) // num_steps\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "      i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "      x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "      x.set_shape([batch_size, num_steps])\n",
    "      return x\n",
    "\n",
    "  def __init__(self, config, data_x, data_y, name=None):\n",
    "    self.epoch_size = ((len(data_x) // config.batch_size) - 1) // config.num_steps\n",
    "    self.input_data = self._make_input_producer(data_x, config.batch_size, config.num_steps, name=name)\n",
    "    self.targets = self._make_input_producer(data_y, config.batch_size, config.num_steps, name=name)\n",
    "\n",
    "\n",
    "class PuctuationModel(object):\n",
    "  \"\"\"The Punctuation training/evaluation model.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, config, input_):\n",
    "    self._input = input_\n",
    "    batch_size = config.batch_size\n",
    "    num_steps = config.num_steps\n",
    "    size = config.hidden_size\n",
    "\n",
    "    def lstm_cell():\n",
    "      return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "    \n",
    "    attn_cell = lstm_cell\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      def attn_cell():\n",
    "        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, size], dtype=tf.float32)\n",
    "    inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "    \n",
    "    if is_training and config.keep_prob < 1:\n",
    "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, inputs, initial_state=self._initial_state)\n",
    "\n",
    "    output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, len(TARGETS)], dtype=tf.float32)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [len(TARGETS)], dtype=tf.float32)\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    self._predictions = tf.argmax(logits, 1)    \n",
    "    self._targets = tf.reshape(input_.targets, [-1])\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(input_.targets, [-1])],\n",
    "        [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = state\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "    self._train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "    self._new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "    self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "  @property\n",
    "  def input(self):\n",
    "    return self._input\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "  \n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state  \n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def predictions(self):\n",
    "    return self._predictions\n",
    "\n",
    "  @property\n",
    "  def targets(self):\n",
    "    return self._targets\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The trainer. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def run_epoch(session, model, num_steps, word_to_id, is_eval=False):\n",
    "  \"\"\"Runs the model on the given data for one epoch.\"\"\"\n",
    "\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  state = session.run(model.initial_state)\n",
    "\n",
    "  fetches = {\n",
    "      \"cost\": model.cost,\n",
    "      \"final_state\": model.final_state,\n",
    "      \"predictions\": model.predictions,\n",
    "      \"targets\": model.targets,\n",
    "  }\n",
    "  if is_eval is False:\n",
    "    fetches[\"train_op\"] = model.train_op\n",
    "\n",
    "  confusion_matrix = np.zeros(shape=(len(TARGETS),len(TARGETS)), dtype=np.int64)\n",
    "  for step in range(model.input.epoch_size):\n",
    "    feed_dict = {}\n",
    "    # Set the state back to model after each run.\n",
    "    for i, (c, h) in enumerate(model.initial_state):\n",
    "      feed_dict[c] = state[i].c\n",
    "      feed_dict[h] = state[i].h\n",
    "\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    cost = vals[\"cost\"]\n",
    "    state = vals[\"final_state\"]\n",
    "    targets = vals[\"targets\"]\n",
    "    predictions = vals['predictions']\n",
    "    \n",
    "    for t, p in zip(targets, predictions):\n",
    "      confusion_matrix[t][p] += 1\n",
    "    \n",
    "    costs += cost\n",
    "    iters += num_steps\n",
    "\n",
    "  if is_eval is True:\n",
    "    for i, t in enumerate(confusion_matrix):\n",
    "      print('%s --- total: %d, correct: %d, accuracy: %.3f, ' % (TARGETS[i], sum(t), t[i], float(t[i]) / sum(t)))\n",
    "      \n",
    "  # Costs are calculated as cross-entropy loss.\n",
    "  # Returns perplexity value (https://en.wikipedia.org/wiki/Perplexity), which is a common measurements on language models.\n",
    "  return np.exp(costs / iters), confusion_matrix\n",
    "\n",
    "\n",
    "def train(train_data_path, validation_data_path, save_path):\n",
    "  \"\"\"Train the model and save a checkpoint at the end.\"\"\"\n",
    "  \n",
    "  word_to_id = build_vocab(train_data_path)\n",
    "  train_data_x, train_data_y = file_to_word_and_punc_ids(train_data_path, word_to_id)\n",
    "  valid_data_x, valid_data_y = file_to_word_and_punc_ids(validation_data_path, word_to_id)\n",
    "  config = TrainingConfig()\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    with tf.name_scope(\"Train\"):\n",
    "      train_input = TrainingInput(config=config, data_x=train_data_x, data_y=train_data_y, name=\"TrainInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        train_model = PuctuationModel(is_training=True, config=config, input_=train_input)\n",
    "      tf.summary.scalar(\"Training_Loss\", train_model.cost)\n",
    "      tf.summary.scalar(\"Learning_Rate\", train_model.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "      valid_input = TrainingInput(config=config, data_x=valid_data_x, data_y=valid_data_y, name=\"ValidInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        valid_model = PuctuationModel(is_training=False, config=config, input_=valid_input)\n",
    "      tf.summary.scalar(\"Validation_Loss\", valid_model.cost)\n",
    "\n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "      for i in range(config.max_max_epoch):\n",
    "        lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "        train_model.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(train_model.lr)))\n",
    "        train_perplexity, _ = run_epoch(session, train_model, config.num_steps, word_to_id)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        valid_perplexity, _ = run_epoch(session, valid_model, config.num_steps, word_to_id, is_eval=True)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "      model_file_prefix = sv.saver.save(session, save_path, global_step=sv.global_step)\n",
    "\n",
    "  word_to_id_file = os.path.join(os.path.dirname(save_path), 'word_to_id.json')\n",
    "  with open(word_to_id_file, 'w') as outfile:\n",
    "    json.dump(word_to_id, outfile)\n",
    "  return model_file_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model directory if it exists so it always trains from beginning.\n",
    "!rm -r -f /content/datalab/punctuation/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training. Training takes about 20 ~ 30 minutes on a n1-standard-1 GCP VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/content/datalab/punctuation/model'\n",
    "saved_model_path = model_dir + '/punctuation'\n",
    "model_file_prefix = train(\n",
    "  '/content/datalab/punctuation/datapreped/train.txt',\n",
    "  '/content/datalab/punctuation/datapreped/valid.txt',\n",
    "  saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In epoch 1, the mode predicted almost everything to be 'n'. It makes sense because vast majority of targets is \"no punctuation\" for each word so betting on that gives good overal accuracy already, although useless.\n",
    "\n",
    "Starting from epoch 2, it learned to predict some '.'. After epoch 10, it could predict about 50% of ','. Only after epoch 15 it started predicting some '?'. Unfortunately, it never predicted '!' well, probably because the difference between '.' and '!' is very subtle. It also had problems predicting ':', maybe because lack of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a tensorboard instance, and you will see the training/validation loss curves, as well as other stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a tensorboard to see the curves in Datalab. \n",
    "\n",
    "from google.datalab.ml import TensorBoard\n",
    "tb = TensorBoard.start(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard is good but the curves are not saved with notebook. We can use Datalab's library to list and plot events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import Summary\n",
    "summary = Summary(model_dir)\n",
    "summary.list_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.plot(event_names=['Train/Training_Loss', 'Valid/Validation_Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the curves above, we got the best validation results around step 4000, and then in some runs a little bit over-fitting after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "At this point, we are done with training, and evaluation starts from a saved checkpoint. We will reuse the `PuctuationModel` defined earlier since evaluation model and training model are mostly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the model with some test data.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def run_eval(model_file_prefix, test_data_path):\n",
    "  \"\"\"Run evaluation on test data.\"\"\"\n",
    "\n",
    "  word_to_id_file = os.path.join(os.path.dirname(model_file_prefix), 'word_to_id.json')\n",
    "  with open(word_to_id_file, 'r') as f:\n",
    "    word_to_id = json.load(f)\n",
    "  test_data_x, test_data_y = file_to_word_and_punc_ids(test_data_path, word_to_id)\n",
    "\n",
    "  eval_config = TrainingConfig()\n",
    "  eval_config.batch_size = 1\n",
    "  eval_config.num_steps = 1\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    with tf.name_scope(\"Test\"):\n",
    "      test_input = TrainingInput(config=eval_config, data_x=test_data_x, data_y=test_data_y, name=\"TestInput\")\n",
    "      with tf.variable_scope(\"Model\", reuse=None):\n",
    "        mtest = PuctuationModel(is_training=False, config=eval_config, input_=test_input)\n",
    "\n",
    "    logdir=os.path.join(os.path.dirname(model_file_prefix), 'eval')        \n",
    "    sv = tf.train.Supervisor(logdir=logdir)\n",
    "    with sv.managed_session() as session:\n",
    "      sv.saver.restore(session, model_file_prefix)\n",
    "      test_perplexity, cm_data = run_epoch(session, mtest, 1, word_to_id, is_eval=True)\n",
    "  return cm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import ConfusionMatrix\n",
    "from pprint import pprint\n",
    "\n",
    "cm_data = run_eval(model_file_prefix, '/content/datalab/punctuation/datapreped/test.txt')\n",
    "pprint(cm_data.tolist())\n",
    "cm = ConfusionMatrix(cm_data, TARGETS)\n",
    "cm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix after removing \"no punctuation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data_puncuations = cm_data.tolist()\n",
    "for i, r in enumerate(cm_data_puncuations):\n",
    "  cm_data_puncuations[i] = r[:-1]\n",
    "cm_data_puncuations = cm_data_puncuations[:-1]\n",
    "ConfusionMatrix(cm_data_puncuations, TARGETS[:-1]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the \",\" are mistakenly predicted as \"no punctuation\", probably because many times either with or without comma is correct in syntax. There are some confusions between \",\" and \".\", meaning that the model \"knows\" it is a break in sentence, but mistakenly chose comma or period. 65% of question marks are predicted correctly. For that we can give credits to LSTM model because it can \"remember\" the beginning of a sentence (which, what, where, etc) even if it is long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Fun time. Let's try generating some puncuations on test data. We'll need to define a \"Prediction Model\". It is a simplified training model, with num_steps and batch_size both being 1, and no loss or training ops. But the model is \"compatible\" with the training model in the sense that they share same variables, and it can load a checkpoint produced in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class PredictModel(object):\n",
    "  \"\"\"The Prediction model.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    self._input = tf.placeholder(shape=[1, 1], dtype=tf.int64)\n",
    "    size = config.hidden_size\n",
    "\n",
    "    def lstm_cell():\n",
    "      return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "    self._initial_state = cell.zero_state(1, tf.float32)\n",
    "    embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, size], dtype=tf.float32)\n",
    "    inputs = tf.nn.embedding_lookup(embedding, self._input)\n",
    "    inputs = tf.unstack(inputs, num=1, axis=1)\n",
    "    outputs, self._final_state = tf.contrib.rnn.static_rnn(cell, inputs, initial_state=self._initial_state)\n",
    "    output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, len(TARGETS)], dtype=tf.float32)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [len(TARGETS)], dtype=tf.float32)\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    self._prediction = tf.argmax(logits, 1)\n",
    "\n",
    "  @property\n",
    "  def input(self):\n",
    "    return self._input\n",
    "  \n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "  \n",
    "  @property\n",
    "  def prediction(self):\n",
    "    return self._prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Predictor that runs the prediction model.\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    \n",
    "  def __init__(self, model_file_prefix):\n",
    "    word_to_id_file = os.path.join(os.path.dirname(model_file_prefix), 'word_to_id.json')\n",
    "    with open(word_to_id_file, 'r') as f:\n",
    "      self._word_to_id = json.load(f)\n",
    "\n",
    "    config = TrainingConfig()    \n",
    "    with tf.Graph().as_default():\n",
    "      with tf.variable_scope(\"Model\", reuse=None):\n",
    "        self._model = PredictModel(config=config)\n",
    "\n",
    "      saver = tf.train.Saver()\n",
    "      self._session = tf.Session()\n",
    "      saver.restore(self._session, model_file_prefix)\n",
    "\n",
    "  def _get_predicted_until_punc(self, min_steps, data_x):\n",
    "\n",
    "    state = self._session.run(self._model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": self._model.final_state,\n",
    "        \"prediction\": self._model.prediction,\n",
    "    }\n",
    "    predicted_puncs = []\n",
    "    step = 0\n",
    "    for x in data_x:\n",
    "      feed_dict = {}\n",
    "      for i, (c, h) in enumerate(self._model.initial_state):\n",
    "        feed_dict[c] = state[i].c\n",
    "        feed_dict[h] = state[i].h\n",
    "      feed_dict[self._model.input] = [[x]]\n",
    "\n",
    "      vals = self._session.run(fetches, feed_dict)\n",
    "      state = vals[\"final_state\"]\n",
    "      prediction = vals[\"prediction\"]\n",
    "      predicted = TARGETS[prediction[0]]\n",
    "      predicted_puncs.append(predicted)\n",
    "      step += 1\n",
    "      if predicted != 'n' and step > min_steps:\n",
    "        break\n",
    "    return predicted_puncs\n",
    "  \n",
    "  def _apply_puncts_to_original(self, original, inserted):\n",
    "    current_index = 0\n",
    "    punc_positions = {}\n",
    "    for w in inserted.split():\n",
    "      if w in PUNCTUATIONS:\n",
    "        punc_positions[current_index] = w\n",
    "      else:\n",
    "        current_index += 1\n",
    "    words = []\n",
    "    for i, w in enumerate(original.split() + ['']):\n",
    "      if i in punc_positions:\n",
    "        words.append(punc_positions[i])\n",
    "      words.append(w)\n",
    "\n",
    "    return ' '.join(words)\n",
    "          \n",
    "  def predict(self, content):\n",
    "    \"\"\"Insert punctuations with given string.\"\"\"\n",
    "\n",
    "    content = content.strip().lower()\n",
    "    for p in PUNCTUATIONS:\n",
    "      content = content.replace(' ' + p, '')\n",
    "    prediction_source = content\n",
    "    prediction_result = ''\n",
    "    \n",
    "    content = '<eos> ' + content + ' <eos>'\n",
    "    min_step = 0\n",
    "    while True:\n",
    "      data_x = content_to_word_ids(content, self._word_to_id)\n",
    "      puncts = self._get_predicted_until_punc(min_step, data_x)\n",
    "      if len(data_x) == len(puncts):\n",
    "        content = content.replace('. <eos> ', '').replace(' <eos>', ' ' + puncts[-1]) + '\\n'\n",
    "        prediction_result = self._apply_puncts_to_original(prediction_source, content)\n",
    "        break\n",
    "      else:\n",
    "        words1 = [self._word_to_id.keys()[self._word_to_id.values().index(data_x[index])] for index in range(len(puncts) - 1)]\n",
    "        indices = [i for i, w in enumerate(words1) if w in PUNCTUATIONS]\n",
    "        for i in indices:\n",
    "          words1[i], words1[i-1] = words1[i-1], words1[i] \n",
    "        words2 = [self._word_to_id.keys()[self._word_to_id.values().index(data_x[index])] for index in range(len(puncts) - 1, len(data_x))]\n",
    "        all_words = words1 + [puncts[-1]] + words2  \n",
    "        content = ' '.join(all_words)  \n",
    "        min_step = len(puncts)          \n",
    "    \n",
    "    return prediction_source, prediction_result\n",
    "\n",
    "  def predict_from_test_file(self, filename, num_random_lines):\n",
    "    \"\"\"given a file from test file, pick some random lines and do prediction.\"\"\"    \n",
    "\n",
    "    num_lines = sum(1 for line in open(filename))\n",
    "    with open(filename) as f:\n",
    "      lines = random.sample(f.readlines(), num_random_lines)\n",
    "    for line in lines:\n",
    "      line = line.strip().lower()\n",
    "      source, predicted = self.predict(line)\n",
    "      yield line, source, predicted\n",
    "\n",
    "  def close(self):\n",
    "    self._session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with three paragraphs. First and second are single sentences, the third one contains multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model_file_prefix)\n",
    "sources = [\n",
    "  'i think it is a report which will for the most part be supported by my group',\n",
    "  'so what is the european union doing about it',\n",
    "  'we must work more rapidly towards achieving the targets stipulated ' + \n",
    "    'in the white paper for renewable energy sources as this would bring ' + \n",
    "    'about a massive reduction in greenhouse gases but in common with others ' + \n",
    "    ' we too are having to endure the greenhouse effect furthermore we should ' + \n",
    "    'utilise an extraordinary budget line since this is an extraordinarily catastrophic situation',\n",
    "]\n",
    "for s in sources:\n",
    "  source, predicted = predictor.predict(s)\n",
    "  print('\\n---SOURCE----\\n' + source)\n",
    "  print('---PREDICTED----\\n' + predicted)\n",
    "\n",
    "predictor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last prediction is actually somewhat incorrect. It should be:\n",
    "\n",
    "`we must work more rapidly towards achieving the targets stipulated in the white paper for renewable energy sources , as this would bring about a massive reduction in greenhouse gases . but in common with others , we too are having to endure the greenhouse effect . furthermore , we should utilise an extraordinary budget line , since this is an extraordinarily catastrophic situation .`\n",
    "\n",
    "It mistakenly predicted the first period where it should be comma. I think we may improve it by showing more words instead of one after the punctuation, or doing it bidirectionally and mix both scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try some data outside our test data (test data and training data are generated from the same data). The first two are common conversational questions, and third is from recent european parliament news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model_file_prefix)\n",
    "sources = [\n",
    "  'how are you',\n",
    "  'where do you see yourself in five years',\n",
    "  'last december the european commission proposed updating the existing customs union with ' + \n",
    "    'turkey and extending bilateral trade relations once negotiations have been completed ' + \n",
    "    'the agreement would still have to be approved by the Parliament before it could enter into force',\n",
    "]\n",
    "for s in sources:\n",
    "  source, predicted = predictor.predict(s)\n",
    "  print('\\n---SOURCE----\\n' + source)\n",
    "  print('---PREDICTED----\\n' + predicted)\n",
    "\n",
    "predictor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convenience, the predictor can pick random sentences from a test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model_file_prefix)\n",
    "for t, s, p in predictor.predict_from_test_file('/content/datalab/punctuation/datapreped/test.txt', 3):\n",
    "  print('\\n---SOURCE----\\n' + s)\n",
    "  print('---PREDICTED----\\n' + p)\n",
    "  print('---TRUTH----\\n' + t)\n",
    "predictor.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorBoard.stop(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
