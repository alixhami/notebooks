{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient training for image classification\n",
    "\n",
    "### _Transfer learning using Inception Package - Local Run Experience_\n",
    "Traditionally, image classification required a very large corpus of training data - often millions of images which may not be available and a long time to train on those images which is expensive and time consuming. That has changed with transfer learning which can be readily used with Cloud ML Engine and without deep knowledge of image classification algorithms using the ML toolbox in Datalab.\n",
    "\n",
    "This notebook codifies the capabilities discussed in this [blog post](https://cloud.google.com/blog/big-data/2016/12/how-to-train-and-classify-images-using-google-cloud-machine-learning-and-cloud-dataflow). In a nutshell, it uses the pre-trained inception model as a starting point and then uses transfer learning to train it further on additional, customer-specific images. For explanation, simple flower images are used. Compared to training from scratch, the training data requirements, time and costs are drastically reduced.\n",
    "\n",
    "This notebook does all operations in the Datalab container without calling CloudML API. Hence, this is called \"local\" operations - though Datalab itself is most often running on a GCE VM. See the corresponding cloud notebook for cloud experience which only adds the --cloud parameter and some config to the local experience commands. The purpose of local work is to do some initial prototyping and debugging on small scale data - often by taking a suitable (say 0.1 - 1%) sample of the full data. The same basic steps can then be repeated with much larger datasets in cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "All data is available under gs://cloud-datalab/sampledata/flower. eval100 is a subset of eval300, which is a subset of eval670. Same for train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content/flowerdata: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /content/flowerdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: 9 files/objects could not be transferred.\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp gs://cloud-datalab/sampledata/flower/* /content/flowerdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define directories for preprocessing, model, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mltoolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2736c476ddf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmltoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatalab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mworker_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/datalab/tmp/flower'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpreprocessed_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/flowerrunlocal'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mltoolbox'"
     ]
    }
   ],
   "source": [
    "import mltoolbox.image.classification as model\n",
    "from google.datalab.ml import *\n",
    "\n",
    "worker_dir = '/content/datalab/tmp/flower'\n",
    "preprocessed_dir = worker_dir + '/flowerrunlocal'\n",
    "model_dir = worker_dir + '/tinyflowermodellocal'\n",
    "prediction_dir = worker_dir + '/flowermodelevallocal'\n",
    "images_dir = worker_dir + '/images'\n",
    "local_train_file = '/content/flowerdata/train200local.csv'\n",
    "local_eval_file = '/content/flowerdata/eval100local.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {images_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get best efficiency, we download the images to local disk, and create our training and evaluation files to reference local path instead of GCS path. Note that the original training files referencing GCS image paths work too, although a bit slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datalab.storage as gcs\n",
    "import os\n",
    "\n",
    "\n",
    "def download_images(input_csv, output_csv, images_dir):\n",
    "  with open(input_csv) as csvfile:\n",
    "    data = list(csv.DictReader(csvfile, fieldnames=['image_url', 'label']))\n",
    "  for x in data:\n",
    "    url = x['image_url']\n",
    "    out_file = os.path.join(images_dir, os.path.basename(url))\n",
    "    with open(out_file, 'w') as f:\n",
    "      f.write(gcs.Item.from_url(url).read_from())\n",
    "    x['image_url'] = out_file\n",
    "\n",
    "  with open(output_csv, 'w') as w:\n",
    "    csv.DictWriter(w, fieldnames=['image_url', 'label']).writerows(data)\n",
    "\n",
    "\n",
    "download_images('/content/flowerdata/train200.csv', local_train_file, images_dir)    \n",
    "download_images('/content/flowerdata/eval100.csv', local_eval_file, images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code can best be illustrated by the comparison below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /content/flowerdata/train200.csv -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {local_train_file} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Preprocessing uses a Dataflow pipeline to convert the image format, resize images, and run the converted image through a pre-trained model to get the features or embeddings. You can also do this step using alternate technologies like Spark or plain Python code if you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell takes ~5 min on a n1-standard-1 VM. Preprocessing the full 3000 images takes about one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of local_train_file, it can take '/content/flowerdata/train200.csv' too, but processing will be slower.\n",
    "train_set = CsvDataSet(local_train_file, schema='image_url:STRING,label:STRING')\n",
    "model.preprocess(train_set, preprocessed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "The next step is to train the inception model with the preprocessed images using transfer learning. Transfer learning retains most of the inception model but replaces the final layer as shown in the image.\n",
    "\n",
    "![inception](https://cloud.google.com/blog/big-data/2016/12/images/148114735559140/image-classification-3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "model.train(preprocessed_dir, 30, 800, model_dir)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run TensorBoard to visualize the completed training. Review accuracy and loss in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_id = TensorBoard.start(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the TF summary events from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary(model_dir)\n",
    "summary.list_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.plot('accuracy')\n",
    "summary.plot('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Let's start with a quick check by taking a couple of images and using the model to predict the type of flower locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "  'gs://cloud-ml-data/img/flower_photos/daisy/15207766_fc2f1d692c_n.jpg',\n",
    "  'gs://cloud-ml-data/img/flower_photos/tulips/6876631336_54bf150990.jpg'\n",
    "]\n",
    "# set show_image to False to not display pictures.\n",
    "model.predict(model_dir, images, show_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "We did a quick test of the model using a few samples. But we need to understand how the model does by evaluating it against much larger amount of labeled data. In the initial preprocessing step, we did set aside enough images for that purpose. Next, we will use normal batch prediction and compare the results with the previously labeled targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following batch prediction and loading of results takes ~3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "bq.Dataset('flower').create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = CsvDataSet(local_eval_file, schema='image_url:STRING,label:STRING')\n",
    "model.batch_predict(eval_set, model_dir, output_bq_table='flower.eval_results_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the results and expected results loaded in a BigQuery table, let's start analyzing the errors and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bq query --name wrong_prediction\n",
    "\n",
    "SELECT * FROM flower.eval_results_local where target != predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_prediction.execute().result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is a common way of comparing the confusion of the model - aggregate data about where the actual result did not match the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix.from_bigquery('flower.eval_results_local').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced analysis can be done using the feature slice view. For the feature slice view, let's define SQL queries that compute accuracy and log loss and then use the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bq query --name accuracy\n",
    "\n",
    "SELECT\n",
    "  target,\n",
    "  SUM(CASE WHEN target=predicted THEN 1 ELSE 0 END) as correct,\n",
    "  COUNT(*) as total,\n",
    "  SUM(CASE WHEN target=predicted THEN 1 ELSE 0 END)/COUNT(*) as accuracy\n",
    "FROM\n",
    "  flower.eval_results_local\n",
    "GROUP BY\n",
    "  target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.execute().result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bq query --name logloss\n",
    "\n",
    "SELECT feature, AVG(-logloss) as logloss, count(*) as count FROM\n",
    "(\n",
    "SELECT feature, CASE WHEN correct=1 THEN LOG(prob) ELSE LOG(1-prob) END as logloss\n",
    "FROM\n",
    "(\n",
    "SELECT\n",
    "target as feature, \n",
    "CASE WHEN target=predicted THEN 1 ELSE 0 END as correct,\n",
    "target_prob as prob\n",
    "FROM flower.eval_results_local))\n",
    "GROUP BY feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSliceView().plot(logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import google.datalab.bigquery as bq\n",
    "\n",
    "TensorBoard.stop(tb_id)\n",
    "bq.Table('flower.eval_results_local').delete()\n",
    "shutil.rmtree(worker_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "In this notebook, we covered local preprocessing, training, prediction and evaluation. We started from data in GCS in csv form plus images; used transfer learning for very fast training and then used BigQuery for model performance analysis. In the next notebook, we will use CloudML APIs that scale a lot better for larger scale. The syntax and analyses will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
