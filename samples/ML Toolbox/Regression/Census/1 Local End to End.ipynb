{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will cover the core parts of the machine learning workflow, running locally within the Google Cloud Datalab environment. Local development and validation, along with using a sample of the full dataset, is recommended as a starting point. This allows for a shorter development-validation iteration cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace Setup\n",
    "\n",
    "The first step is to setup the workspace that we will use within this notebook - the python libraries, and the local directory containing the data inputs and outputs produced over the course of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mltoolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf1e7f5ce032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmltoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mltoolbox'"
     ]
    }
   ],
   "source": [
    "import google.datalab.ml as ml\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plot\n",
    "import mltoolbox.regression.dnn as regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local development workspace will be in `/content/datalab/workspace/census` by default.\n",
    "\n",
    "Note that the `/content/datalab` directory is physically located within the data disk mounted into the Datalab instance, but outside of the git repository containing notebooks, which makes it suitable for storing data files and generated files that are useful to keep around while you are working on a project, but do not belong in the source repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_path = '/content/datalab/workspace/census'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you have previously run this notebook, and want to start from scratch, then run the next cell to delete and create the workspace directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {workspace_path} && mkdir {workspace_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will copy the data into this workspace. Generally, in your own work, you will need to create a representative sample dataset to use for local development, while leaving the full dataset to use when running on the service. For purposes of the sample, which uses a relatively small dataset, we'll copy it down in entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -q cp gs://cloud-datalab-samples/census/ss14psd.csv {workspace_path}/data/census.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {workspace_path}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Its a good idea to load data and inspect it to build an understanding of the structure, as well as preparation steps that will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(os.path.join(workspace_path, 'data/census.csv'), dtype=str)\n",
    "print '%d rows' % len(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The census data contains a large number of columns. Only a few are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Transformations\n",
    "\n",
    "The raw census data requires a number of transformations before it is usable for machine learning:\n",
    "\n",
    "1. Apply understanding of the domain and the problem to determine which data to include or join, as well as which data to filter out if it is adding noise. In the case of census, we'll pick just a few of the many columns present in the dataset.\n",
    "2. Handle missing values, or variations in formatting.\n",
    "3. Apply other transformations in support of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is packaged as a function that can be reused if you need to apply to future\n",
    "# datasets, esp. to prediction data, to ensure consistent transformations are applied.\n",
    "\n",
    "def transform_data(df):\n",
    "  interesting_columns = ['WAGP','SERIALNO','AGEP','COW','ESP','ESR','FOD1P','HINS4','INDP',\n",
    "                         'JWMNP', 'JWTR', 'MAR', 'POWPUMA', 'PUMA', 'RAC1P', 'SCHL',\n",
    "                         'SCIENGRLP', 'SEX', 'WKW']\n",
    "  df = df[interesting_columns]\n",
    "  \n",
    "  # Replace whitespace with NaN, and NaNs with empty string\n",
    "  df = df.replace('\\s+', np.nan, regex=True).fillna('')\n",
    "\n",
    "  # Filter out the rows without an income, i.e. there is no target value to learn from\n",
    "  df = df[df.WAGP != '']\n",
    "  \n",
    "  # Convert the wage value into units of 1000. So someone making an income from wages\n",
    "  # of $23200 will have it encoded as 23.2\n",
    "  df['WAGP'] = df.WAGP.astype(np.int64) / 1000.0\n",
    "\n",
    "  # Filter out rows with income values we don't care about, i.e. outliers\n",
    "  # Filter out rows with less than 10K and more than 150K\n",
    "  df = df[(df.WAGP >= 10.0) & (df.WAGP < 150.0)]\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = transform_data(df_data)\n",
    "print '%d rows' % len(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataSets\n",
    "\n",
    "Once the data is ready, the next step is to split data into training and evaluation datasets. In this sample, rows are split randomly in an 80/20 manner. Additionally, the schema is also saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(df):\n",
    "  fields = []\n",
    "  for name, dtype in zip(df.columns, df.dtypes):\n",
    "    if dtype in (np.str, np.object):\n",
    "      # Categorical columns should have type 'STRING'\n",
    "      fields.append({'name': name, 'type': 'STRING'})\n",
    "    elif dtype in (np.int32, np.int64, np.float32, np.float64):\n",
    "      # Numerical columns have type 'FLOAT'\n",
    "      fields.append({'name': name, 'type': 'FLOAT'})\n",
    "    else:\n",
    "      raise ValueError('Unsupported column type \"%s\" in column \"%s\"' % (str(dtype), name))\n",
    "  return fields\n",
    "\n",
    "def create_datasets(df):\n",
    "  # Numbers in the range of [0, 1)\n",
    "  random_values = np.random.rand(len(df))\n",
    "\n",
    "  # Split data into %80, 20% partitions\n",
    "  df_train = df[random_values < 0.8]\n",
    "  df_eval = df[random_values >= 0.8]\n",
    "\n",
    "  return df_train, df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_eval = create_datasets(df_data)\n",
    "schema = create_schema(df_data)\n",
    "\n",
    "training_data_path = os.path.join(workspace_path, 'data/train.csv')\n",
    "eval_data_path = os.path.join(workspace_path, 'data/eval.csv')\n",
    "schema_path = os.path.join(workspace_path, 'data/schema.json')\n",
    "\n",
    "df_train.to_csv(training_data_path, header=False, index=False)\n",
    "df_eval.to_csv(eval_data_path, header=False, index=False)\n",
    "\n",
    "with open(schema_path, 'w') as f:\n",
    "  f.write(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {workspace_path}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create DataSet objects which are reference to one or more files identified by a path (or path pattern) along with associated schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ml.CsvDataSet(file_pattern=training_data_path, schema_file=schema_path)\n",
    "eval_data = ml.CsvDataSet(file_pattern=eval_data_path, schema_file=schema_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data\n",
    "\n",
    "When building a model, a number of pieces of information about the training data are required - for example, the list of entries or vocabulary of a categorical/discrete column, or aggregate statistics like min and max for numerical columns. These require a full pass over the training data, and is usually done once, and needs to be repeated once if you change the schema in a future iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = os.path.join(workspace_path, 'analysis')\n",
    "\n",
    "regression.analyze(dataset=train_data, output_dir=analysis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of analysis is a stats file that contains analysis from the numerical columns, and a vocab file from each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {analysis_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "All the data is in place to start training. A model learns to predict the target value (the income, 'WAGP'), based on the different pieces of input data (the various columns or features). The target and inputs are defined as features derived from the input data by applying a set of transforms to the columns.\n",
    "\n",
    "Additionally there is a special key column - this is any column in the data that can be used to uniquely identify instances. The value of this column is ignored during training, but this value is quite useful when using the resulting model during prediction as discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "  \"WAGP\": {\"transform\": \"target\"},\n",
    "  \"SERIALNO\": {\"transform\": \"key\"},\n",
    "  \"AGEP\": {\"transform\": \"embedding\", \"embedding_dim\": 2},  # Age\n",
    "  \"COW\": {\"transform\": \"one_hot\"},                         # Class of worker\n",
    "  \"ESP\": {\"transform\": \"embedding\", \"embedding_dim\": 2},   # Employment status of parents\n",
    "  \"ESR\": {\"transform\": \"one_hot\"},                         # Employment status\n",
    "  \"FOD1P\": {\"transform\": \"embedding\", \"embedding_dim\": 3}, # Field of degree\n",
    "  \"HINS4\": {\"transform\": \"one_hot\"},                       # Medicaid\n",
    "  \"INDP\": {\"transform\": \"embedding\", \"embedding_dim\": 5},  # Industry\n",
    "  \"JWMNP\": {\"transform\": \"embedding\", \"embedding_dim\": 2}, # Travel time to work\n",
    "  \"JWTR\": {\"transform\": \"one_hot\"},                        # Transportation\n",
    "  \"MAR\": {\"transform\": \"one_hot\"},                         # Marital status\n",
    "  \"POWPUMA\": {\"transform\": \"one_hot\"},                     # Place of work\n",
    "  \"PUMA\": {\"transform\": \"one_hot\"},                        # Area code\n",
    "  \"RAC1P\": {\"transform\": \"one_hot\"},                       # Race\n",
    "  \"SCHL\": {\"transform\": \"one_hot\"},                        # School\n",
    "  \"SCIENGRLP\": {\"transform\": \"one_hot\"},                   # Science\n",
    "  \"SEX\": {\"transform\": \"one_hot\"},\n",
    "  \"WKW\": {\"transform\": \"one_hot\"}                          # Weeks worked\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = os.path.join(workspace_path, 'training')\n",
    "regression.train(train_dataset=train_data, eval_dataset=eval_data,\n",
    "                 output_dir=training_path,\n",
    "                 analysis_dir=analysis_path,\n",
    "                 features=features,\n",
    "                 max_steps=2000,\n",
    "                 layer_sizes=[5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "A training job produces various summary events containing values of metrics (eg. throughput and loss) over the course of its execution. These events can be observed in TensorBoard while the job executes or after it is executed.\n",
    "\n",
    "In this sample, training was short, and has completed. In the general case, especially for longer cloud training jobs, it is more interesting to launch TensorBoard while the training job continues to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_pid = ml.TensorBoard.start(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.TensorBoard.stop(tensorboard_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trained Model\n",
    "\n",
    "It is interesting to get a sense of all the outputs produced during training, in addition to the summary event files, visualized in the previous step. In particular, note that the model is produced in a `model` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -R {training_path}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Once a model has been trained, it is necessary to evaluate it and understand how well it is performing. In order to evaluate a model, batch prediction jobs can be run against the one or more evaluation datasets that you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = os.path.join(workspace_path, 'evaluation')\n",
    "\n",
    "# Note the use of evaluation mode (as opposed to prediction mode). This is used to indicate the data being\n",
    "# predicted on contains a target value column (prediction data is missing that column).\n",
    "regression.batch_predict(training_dir=training_path,\n",
    "                         prediction_input_file=eval_data_path,\n",
    "                         output_dir=evaluation_path,\n",
    "                         output_format='json',\n",
    "                         mode='evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l {evaluation_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_json(os.path.join(evaluation_path, 'predictions-00000-of-00001.json'), lines=True)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(df_eval['target'], df_eval['predicted'])\n",
    "rmse = math.sqrt(mse)\n",
    "print 'Root Mean Squared Error: %.3f' % rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['error'] = df_eval['predicted'] - df_eval['target']\n",
    "_ = plot.hist(df_eval['error'], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error and distribution of errors indicates how the model is performing at an aggregate level as well as indicative of the span of error values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now that a model has been trained, and saved on-disk, it can be reloaded using TensorFlow, and be used to produce predictions, i.e. produce the income value given a set of new instances, or features that were not previously present in the training data. This mechanism can also help validate the model - it can be used to produce predictions for one or more evaluation datasets.\n",
    "\n",
    "Note that prediction data must be of the same type (input format, and order of columns) as the data that was used for training. The only difference is the first column, the target income value, is absent.\n",
    "\n",
    "Since the model is a regression model, a single value is the output of the prediction.\n",
    "\n",
    "Also note that second column in our schema was specified as a key column. This value of the key will accompany the output values, so they can be joined with the input instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%file {workspace_path}/data/prediction.csv\n",
    "SERIALNO,AGEP,COW,ESP,ESR,FOD1P,HINS4,INDP,JWMNP,JWTR,MAR,POWPUMA,PUMA,RAC1P,SCHL,SCIENGRLP,SEX,WKW\n",
    "490,64,2,0,1,0,2,8090,015,01,1,00590,00500,1,18,0,2,1\n",
    "1225,32,5,0,4,5301,2,9680,015,01,1,00100,00100,1,21,2,1,1\n",
    "1226,30,1,0,1,0,2,8680,020,01,1,00100,00100,1,16,0,2,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = pd.read_csv(os.path.join(workspace_path, 'data/prediction.csv'))\n",
    "df_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = regression.predict(training_dir=training_path, data=df_instances)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the instances DataFrame using the SERIALNO column, and join the predictions\n",
    "# DataFrame using the same column.\n",
    "df_instances.set_index(keys=['SERIALNO'], inplace=True)\n",
    "df_predictions.set_index(keys=['SERIALNO'], inplace=True)\n",
    "\n",
    "df_data = df_predictions.join(other=df_instances)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "This notebook covered key stages of the workflow locally - data preparation, data analysis, training, and prediction. Once there is a working model, the next step is to use the full dataset, and scale to much larger data volumes by performing these steps in cloud using BigQuery, Machine Learning Engine, and Dataflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-notebooks",
   "language": "python",
   "name": "datalab-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
